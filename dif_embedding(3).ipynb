{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:16:43.391245Z",
          "iopub.execute_input": "2025-01-27T12:16:43.391762Z",
          "iopub.status.idle": "2025-01-27T12:16:48.581718Z",
          "shell.execute_reply.started": "2025-01-27T12:16:43.391714Z",
          "shell.execute_reply": "2025-01-27T12:16:48.580783Z"
        },
        "id": "N6zaOnFfm8Cp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word, TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:19:01.814192Z",
          "iopub.execute_input": "2025-01-27T12:19:01.815219Z",
          "iopub.status.idle": "2025-01-27T12:19:07.823294Z",
          "shell.execute_reply.started": "2025-01-27T12:19:01.815184Z",
          "shell.execute_reply": "2025-01-27T12:19:07.822590Z"
        },
        "id": "Becpp-Msm8Cu",
        "outputId": "b1841404-1989-4bdf-aa71-8d01beaebf21"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_rows', 55)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:21:07.079334Z",
          "iopub.execute_input": "2025-01-27T12:21:07.080027Z",
          "iopub.status.idle": "2025-01-27T12:21:18.645338Z",
          "shell.execute_reply.started": "2025-01-27T12:21:07.079995Z",
          "shell.execute_reply": "2025-01-27T12:21:18.644654Z"
        },
        "id": "O1TNG2pKm8Cw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Завантаження повного датасету та препроцесинг**"
      ],
      "metadata": {
        "id": "XeHZ_MXLnI7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Завантаження датасету\n",
        "dataset = load_dataset('mavinsao/reddit-mental-illness-82')\n",
        "df_tr = dataset['train'].to_pandas()\n",
        "df_val = dataset['validation'].to_pandas()\n",
        "df_ts = dataset['test'].to_pandas()\n",
        "df = pd.concat([df_tr, df_val, df_ts], axis=0, ignore_index=True)\n",
        "\n",
        "# Перетворення тексту на нижній регістр\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Фільтрація рядків за наявністю ключових слів\n",
        "keywords = [\n",
        "    \"think i have\", \"i think i have\", \"i think it might be\", \"i think i could have\",\n",
        "    \"might have\", \"i might have\", \"might be\", \"feel like i have\", \"i feel like i have\",\n",
        "    \"feels like i have\", \"self-diagnose\", \"self-diagnosed\", \"i've self-diagnosed\",\n",
        "    \"unsure if i have\", \"i'm unsure if\", \"unsure if this is\", \"wonder if i have\",\n",
        "    \"i wonder if i have\", \"wonder if it's\", \"symptoms of\", \"i have symptoms of\",\n",
        "    \"experiencing symptoms of\", \"suspect i have\", \"i suspect i have\", \"i suspect it's\",\n",
        "    \"probably have\", \"i probably have\", \"i think i probably have\", \"could be\", \"it could be\",\n",
        "    \"seems like i have\", \"it seems like i have\", \"it seems like\", \"not diagnosed but\",\n",
        "    \"i am not diagnosed but\", \"i haven't been diagnosed but\"\n",
        "]\n",
        "\n",
        "df = df[~df['text'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "\n",
        "# Розбиття тексту на заголовок і основний текст\n",
        "def split_text(row):\n",
        "    if ':' in row:\n",
        "        parts = row.split(':', 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "    return None, row.strip()\n",
        "\n",
        "df[['title', 'main_text']] = df['text'].apply(split_text).apply(pd.Series)\n",
        "print(df[['title', 'main_text', 'label']])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:21:26.152378Z",
          "iopub.execute_input": "2025-01-27T12:21:26.153565Z",
          "iopub.status.idle": "2025-01-27T12:22:07.609105Z",
          "shell.execute_reply.started": "2025-01-27T12:21:26.153528Z",
          "shell.execute_reply": "2025-01-27T12:22:07.608133Z"
        },
        "colab": {
          "referenced_widgets": [
            "a2fe68d39ab74885a4bb14d3c714afa1",
            "d3c73c29fae84153a0a8a471f36ae387",
            "75e9249b82d74bf69e9ed2867530dad7",
            "ae9359f836a9442db4f37d867ae90628",
            "f8220d71f37c46feb3edfa4eda4ce714",
            "786e20de65bf478094948d0b3b53670c",
            "59c1b83a33ad49aab95f83a2b573654b"
          ]
        },
        "id": "gg1yTvWOm8Cx",
        "outputId": "828c62b4-730c-472a-b0be-e89c64e6c934"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/755 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2fe68d39ab74885a4bb14d3c714afa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/24.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3c73c29fae84153a0a8a471f36ae387"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/3.03M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75e9249b82d74bf69e9ed2867530dad7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/3.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae9359f836a9442db4f37d867ae90628"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/42113 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8220d71f37c46feb3edfa4eda4ce714"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/5264 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "786e20de65bf478094948d0b3b53670c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/5265 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59c1b83a33ad49aab95f83a2b573654b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "                                                   title  \\\n0                              fantasizing about your fp   \n1      this is a support subreddit for people with a ...   \n2                        scared of my psychotic symptoms   \n4      i feel like a sick animal that needs to be put...   \n5      is constantly checking ocd subreddits a compul...   \n...                                                  ...   \n52637  i got diagnosed with pure o, and finding it di...   \n52638                 where can i read up on propaganda?   \n52639  need help \"keeping thoughts straight,\" or, mai...   \n52640  does anyone else feel like none of their exper...   \n52641                         wish me luck at work today   \n\n                                               main_text  label  \n0      do you do it? what do you fantasize about? : w...      3  \n1      it's not for posting how infuriating the uneve...      6  \n2      i'm trying to keep reminding myself that what ...      2  \n4      i had a cat for 18 years. at the end of her li...      4  \n5      i kid you not, when i have the thoughts, i’m c...      6  \n...                                                  ...    ...  \n52637  i am 27, and spent most of my life worried abo...      6  \n52638  i want to learn about the tactics and psycholo...      5  \n52639  i've always been crap at math or abstract conc...      0  \n52640  i've had a lot of people tell me i have anxiet...      1  \n52641  oh god my anxiety is killing to me today. i’m ...      1  \n\n[48084 rows x 3 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Функція очищення тексту\n",
        "def clean_text(text):\n",
        "    text = text.str.lower()\n",
        "    text = text.str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    text = text.str.replace(\"\\n\", '', regex=True)\n",
        "    text = text.str.replace('\\d', '', regex=True)\n",
        "    text = text.str.replace(r'\\[.*?\\]', '', regex=True)\n",
        "    text = text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
        "    text = text.str.replace(r'<.*?>+', '', regex=True)\n",
        "    text = text.str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
        "    return text\n",
        "\n",
        "# Функція видалення стоп-слів\n",
        "# custom_stopwords = {'ocd', 'anxiety', 'adhd', 'ptsd', 'bpd', 'depression', 'bipolar'}\n",
        "custom_stopwords = {}\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(custom_stopwords)\n",
        "    text = text.apply(lambda x: \" \".join(word for word in str(x).split() if word.lower() not in stop_words))\n",
        "    return text\n",
        "\n",
        "# Функція лематизації\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "# Застосування функцій до обох стовпців\n",
        "def process_text_columns(df, columns):\n",
        "    for col in columns:\n",
        "        df[col] = clean_text(df[col])\n",
        "        df[col] = remove_stopwords(df[col])\n",
        "        delete = pd.Series(' '.join(df[col]).split()).value_counts()[-1000:]\n",
        "        df[col] = df[col].apply(lambda x: \" \".join(word for word in x.split() if word.lower() not in delete))\n",
        "        df[col] = df[col].apply(lemmatize_sentence)\n",
        "    return df\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Застосування функцій до стовпців 'title' і 'main_text'\n",
        "df = process_text_columns(df, ['title', 'main_text'])\n",
        "\n",
        "# Результат\n",
        "print(df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:22:21.024538Z",
          "iopub.execute_input": "2025-01-27T12:22:21.025230Z",
          "iopub.status.idle": "2025-01-27T12:33:45.760109Z",
          "shell.execute_reply.started": "2025-01-27T12:22:21.025198Z",
          "shell.execute_reply": "2025-01-27T12:33:45.759072Z"
        },
        "id": "b8e-a7fHm8Cy",
        "outputId": "4bdf27cd-41d9-4ca0-da63-a906e6e9ec78"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                                                    text  label  \\\n0      fantasizing about your fp: do you do it? what ...      3   \n1      this is a support subreddit for people with a ...      6   \n2      scared of my psychotic symptoms : i'm trying t...      2   \n4      i feel like a sick animal that needs to be put...      4   \n5      is constantly checking ocd subreddits a compul...      6   \n...                                                  ...    ...   \n52637  i got diagnosed with pure o, and finding it di...      6   \n52638  where can i read up on propaganda? : i want to...      5   \n52639  need help \"keeping thoughts straight,\" or, mai...      0   \n52640  does anyone else feel like none of their exper...      1   \n52641  wish me luck at work today : oh god my anxiety...      1   \n\n                                                   title  \\\n0                                           fantasize fp   \n1                support subreddit people mental illness   \n2                               scared psychotic symptom   \n4                         feel like sick animal need put   \n5              constantly check ocd subreddit compulsion   \n...                                                  ...   \n52637  get diagnose pure finding difficult come term ...   \n52638                                    read propaganda   \n52639  need help keep thought straight maintain figur...   \n52640        anyone else feel like none experience valid   \n52641                               wish luck work today   \n\n                                               main_text  \n0      fantasize fp attract fp romantic way fantasize...  \n1      post infuriate uneven floor tile satisfy candy...  \n2      I m try keep remind I m think believe be not r...  \n4      cat year end life lose mental faculty lethargi...  \n5      kid thought I m constantly check ocd subreddit...  \n...                                                  ...  \n52637  spend life worry weird different hate judge el...  \n52638  want learn tactic psychology go along history ...  \n52639  I ve always crap math abstract concept can not...  \n52640  I ve lot people tell anxiety do not think they...  \n52641  oh god anxiety kill today I m work get ready c...  \n\n[48084 rows x 4 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Об'єднання заголовку і основного тексту в один стовпець\n",
        "df['text'] = df['title'].str.strip(':') + ' ' + df['main_text']\n",
        "# df['text'] = df['main_text']\n",
        "# Розподіл на X (тексти) та y (мітки)\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "# Ініціалізація RandomUnderSampler для балансування класів\n",
        "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Застосування undersampling\n",
        "X_res, y_res = undersampler.fit_resample(X.values.reshape(-1, 1), y)\n",
        "print(\"Розподіл класів після балансування:\")\n",
        "print(pd.Series(y_res).value_counts())\n",
        "balanced_df = pd.DataFrame({\n",
        "    'text': X_res.flatten(),\n",
        "    'label': y_res\n",
        "})\n",
        "\n",
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "print(\"\\nРозподіл класів у тренувальному наборі:\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"\\nРозподіл класів у тестовому наборі:\")\n",
        "print(test_df['label'].value_counts())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T12:34:34.331437Z",
          "iopub.execute_input": "2025-01-27T12:34:34.332160Z",
          "iopub.status.idle": "2025-01-27T12:34:34.538779Z",
          "shell.execute_reply.started": "2025-01-27T12:34:34.332127Z",
          "shell.execute_reply": "2025-01-27T12:34:34.537839Z"
        },
        "id": "h_gKeVSum8Cy",
        "outputId": "c95f1b78-0d56-4e21-9aa5-8baff225b672"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Розподіл класів після балансування:\nlabel\n0    4086\n1    4086\n2    4086\n3    4086\n4    4086\n5    4086\n6    4086\n7    4086\nName: count, dtype: int64\n\nРозподіл класів у тренувальному наборі:\nlabel\n5    3335\n0    3296\n1    3279\n4    3264\n6    3264\n3    3260\n2    3228\n7    3224\nName: count, dtype: int64\n\nРозподіл класів у тестовому наборі:\nlabel\n7    862\n2    858\n3    826\n6    822\n4    822\n1    807\n0    790\n5    751\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LogisticRegression**"
      ],
      "metadata": {
        "id": "3WzUBhHPnkPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "JpX9pQNVnouA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Трансформація тексту в TF-IDF вектори\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(train_df['text'])\n",
        "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "# Логістична регресія\n",
        "logistic_regression_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred = logistic_regression_classifier.predict(X_test)\n",
        "print(\"Test Set Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:32:39.861838Z",
          "iopub.execute_input": "2025-01-20T21:32:39.862218Z",
          "iopub.status.idle": "2025-01-20T21:33:15.938475Z",
          "shell.execute_reply.started": "2025-01-20T21:32:39.862187Z",
          "shell.execute_reply": "2025-01-20T21:33:15.937194Z"
        },
        "id": "VOx151eJm8Cz",
        "outputId": "66176b57-269c-41c1-f05e-0d1c2e451046"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Test Set Evaluation:\nAccuracy: 0.7872438054450902\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       790\n           1       0.76      0.74      0.75       807\n           2       0.79      0.70      0.74       858\n           3       0.73      0.69      0.71       826\n           4       0.63      0.78      0.69       822\n           5       0.88      0.98      0.93       751\n           6       0.90      0.81      0.85       822\n           7       0.83      0.79      0.81       862\n\n    accuracy                           0.79      6538\n   macro avg       0.79      0.79      0.79      6538\nweighted avg       0.79      0.79      0.79      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "0oD9Y-FknyW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[text.split() for text in train_df['text']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "def text_to_avg_vector(text, model):\n",
        "    words = text.split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([text_to_avg_vector(text, word2vec_model) for text in train_df['text']])\n",
        "X_test_w2v = np.array([text_to_avg_vector(text, word2vec_model) for text in test_df['text']])\n",
        "logistic_regression_classifier_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_classifier_w2v.fit(X_train_w2v, y_train)\n",
        "\n",
        "y_test_pred_w2v = logistic_regression_classifier_w2v.predict(X_test_w2v)\n",
        "print(\"Word2Vec Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_w2v)}\")\n",
        "print(classification_report(y_test, y_test_pred_w2v))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:25:55.149342Z",
          "iopub.execute_input": "2025-01-20T21:25:55.150107Z",
          "iopub.status.idle": "2025-01-20T21:26:34.937852Z",
          "shell.execute_reply.started": "2025-01-20T21:25:55.150056Z",
          "shell.execute_reply": "2025-01-20T21:26:34.936749Z"
        },
        "id": "PxtyP02Lm8C0",
        "outputId": "9315b22b-88fa-49df-a6e5-af04ed4f388d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Word2Vec Evaluation:\nAccuracy: 0.7049556439278066\n              precision    recall  f1-score   support\n\n           0       0.68      0.74      0.70       790\n           1       0.71      0.67      0.69       807\n           2       0.67      0.59      0.63       858\n           3       0.59      0.55      0.57       826\n           4       0.56      0.64      0.59       822\n           5       0.94      0.96      0.95       751\n           6       0.83      0.76      0.79       822\n           7       0.72      0.76      0.74       862\n\n    accuracy                           0.70      6538\n   macro avg       0.71      0.71      0.71      6538\nweighted avg       0.71      0.70      0.70      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "sBHk3YBgn4sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "fasttext_model = FastText(sentences=[text.split() for text in train_df['text']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "X_train_fasttext = np.array([text_to_avg_vector(text, fasttext_model) for text in train_df['text']])\n",
        "X_test_fasttext = np.array([text_to_avg_vector(text, fasttext_model) for text in test_df['text']])\n",
        "logistic_regression_classifier_fasttext = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_classifier_fasttext.fit(X_train_fasttext, y_train)\n",
        "y_test_pred_fasttext = logistic_regression_classifier_fasttext.predict(X_test_fasttext)\n",
        "\n",
        "print(\"FastText Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_fasttext)}\")\n",
        "print(classification_report(y_test, y_test_pred_fasttext))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:26:48.210915Z",
          "iopub.execute_input": "2025-01-20T21:26:48.211610Z",
          "iopub.status.idle": "2025-01-20T21:27:52.010401Z",
          "shell.execute_reply.started": "2025-01-20T21:26:48.211573Z",
          "shell.execute_reply": "2025-01-20T21:27:52.009174Z"
        },
        "id": "CriyLwmwm8C0",
        "outputId": "23581747-222d-4c0b-b14e-794a84be141a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "FastText Evaluation:\nAccuracy: 0.7018966044661976\n              precision    recall  f1-score   support\n\n           0       0.66      0.77      0.71       790\n           1       0.68      0.65      0.67       807\n           2       0.66      0.55      0.60       858\n           3       0.61      0.58      0.59       826\n           4       0.56      0.64      0.60       822\n           5       0.92      0.95      0.93       751\n           6       0.83      0.78      0.80       822\n           7       0.72      0.73      0.72       862\n\n    accuracy                           0.70      6538\n   macro avg       0.71      0.71      0.70      6538\nweighted avg       0.70      0.70      0.70      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "rxDNT0tPn9y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Трансформація тексту в Bag of Words вектори\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['text'])\n",
        "X_test_bow = bow_vectorizer.transform(test_df['text'])\n",
        "logistic_regression_classifier_bow = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_classifier_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "y_test_pred_bow = logistic_regression_classifier_bow.predict(X_test_bow)\n",
        "print(\"Bag of Words Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_bow)}\")\n",
        "print(classification_report(y_test, y_test_pred_bow))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:28:41.197139Z",
          "iopub.execute_input": "2025-01-20T21:28:41.197494Z",
          "iopub.status.idle": "2025-01-20T21:30:28.577799Z",
          "shell.execute_reply.started": "2025-01-20T21:28:41.197464Z",
          "shell.execute_reply": "2025-01-20T21:30:28.574144Z"
        },
        "id": "a0YZPD4Om8C1",
        "outputId": "cc6feef0-85fe-4221-ab3e-45f06f23a786"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Bag of Words Evaluation:\nAccuracy: 0.7577240746405629\n              precision    recall  f1-score   support\n\n           0       0.82      0.78      0.80       790\n           1       0.70      0.70      0.70       807\n           2       0.71      0.70      0.70       858\n           3       0.69      0.67      0.68       826\n           4       0.60      0.71      0.65       822\n           5       0.94      0.94      0.94       751\n           6       0.84      0.81      0.82       822\n           7       0.81      0.77      0.79       862\n\n    accuracy                           0.76      6538\n   macro avg       0.76      0.76      0.76      6538\nweighted avg       0.76      0.76      0.76      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBClassifier**"
      ],
      "metadata": {
        "id": "odjza0MFoHRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "kFtLtNHfoKNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(train_df['text'])\n",
        "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "# XGBoost\n",
        "xgb_classifier = XGBClassifier(\n",
        "    max_depth=3,\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "y_test_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "print(classification_report(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:36:29.729807Z",
          "iopub.execute_input": "2025-01-20T21:36:29.730647Z",
          "iopub.status.idle": "2025-01-20T21:40:32.750014Z",
          "shell.execute_reply.started": "2025-01-20T21:36:29.730612Z",
          "shell.execute_reply": "2025-01-20T21:40:32.748550Z"
        },
        "id": "hTniavOhm8C2",
        "outputId": "39bdf04c-d314-4cfc-c5f0-6e855bdd822f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Test Set Evaluation:\nAccuracy: 0.7837259100642399\n              precision    recall  f1-score   support\n\n           0       0.85      0.80      0.82       790\n           1       0.77      0.76      0.77       807\n           2       0.84      0.69      0.76       858\n           3       0.73      0.68      0.70       826\n           4       0.59      0.78      0.67       822\n           5       0.81      0.97      0.88       751\n           6       0.89      0.82      0.85       822\n           7       0.87      0.79      0.83       862\n\n    accuracy                           0.78      6538\n   macro avg       0.79      0.79      0.79      6538\nweighted avg       0.79      0.78      0.78      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "zGiYV-yooYL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Трансформація тексту в BoW\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['text'])\n",
        "X_test_bow = bow_vectorizer.transform(test_df['text'])\n",
        "xgb_classifier_bow = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1, random_state=42, use_label_encoder=False)\n",
        "xgb_classifier_bow.fit(X_train_bow, y_train)\n",
        "y_test_pred_bow = xgb_classifier_bow.predict(X_test_bow)\n",
        "print(\"Bag of Words with XGBoost Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_bow)}\")\n",
        "print(classification_report(y_test, y_test_pred_bow))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:47:14.631659Z",
          "iopub.execute_input": "2025-01-20T21:47:14.632031Z",
          "iopub.status.idle": "2025-01-20T21:48:01.698932Z",
          "shell.execute_reply.started": "2025-01-20T21:47:14.631999Z",
          "shell.execute_reply": "2025-01-20T21:48:01.698271Z"
        },
        "id": "9uhmGNhCm8C2",
        "outputId": "101f498a-3044-4d9e-b8d6-adb95d1be9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Bag of Words with XGBoost Evaluation:\nAccuracy: 0.7812786784949526\n              precision    recall  f1-score   support\n\n           0       0.85      0.78      0.82       790\n           1       0.78      0.75      0.77       807\n           2       0.86      0.68      0.76       858\n           3       0.73      0.69      0.71       826\n           4       0.60      0.77      0.68       822\n           5       0.75      0.98      0.85       751\n           6       0.90      0.82      0.86       822\n           7       0.86      0.80      0.83       862\n\n    accuracy                           0.78      6538\n   macro avg       0.79      0.78      0.78      6538\nweighted avg       0.79      0.78      0.78      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "8lBY-oDJodHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Тренування Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=[text.split() for text in train_df['text']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def text_to_avg_vector(text, model):\n",
        "    words = text.split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([text_to_avg_vector(text, word2vec_model) for text in train_df['text']])\n",
        "X_test_w2v = np.array([text_to_avg_vector(text, word2vec_model) for text in test_df['text']])\n",
        "xgb_classifier_w2v = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1, random_state=42, use_label_encoder=False)\n",
        "xgb_classifier_w2v.fit(X_train_w2v, y_train)\n",
        "y_test_pred_w2v = xgb_classifier_w2v.predict(X_test_w2v)\n",
        "print(\"Word2Vec with XGBoost Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_w2v)}\")\n",
        "print(classification_report(y_test, y_test_pred_w2v))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:50:23.733121Z",
          "iopub.execute_input": "2025-01-20T21:50:23.733980Z",
          "iopub.status.idle": "2025-01-20T21:50:52.856229Z",
          "shell.execute_reply.started": "2025-01-20T21:50:23.733943Z",
          "shell.execute_reply": "2025-01-20T21:50:52.854035Z"
        },
        "id": "o7f55EWIm8C2",
        "outputId": "bfd5b7a1-76f5-4133-aa7d-e07b378fbeb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Word2Vec with XGBoost Evaluation:\nAccuracy: 0.6495870296726828\n              precision    recall  f1-score   support\n\n           0       0.63      0.69      0.66       790\n           1       0.62      0.60      0.61       807\n           2       0.58      0.51      0.54       858\n           3       0.52      0.49      0.51       826\n           4       0.51      0.59      0.55       822\n           5       0.93      0.95      0.94       751\n           6       0.74      0.69      0.72       822\n           7       0.69      0.69      0.69       862\n\n    accuracy                           0.65      6538\n   macro avg       0.65      0.65      0.65      6538\nweighted avg       0.65      0.65      0.65      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "oTnI3wIWophx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "# Тренування FastText\n",
        "fasttext_model = FastText(sentences=[text.split() for text in train_df['text']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "X_train_fasttext = np.array([text_to_avg_vector(text, fasttext_model) for text in train_df['text']])\n",
        "X_test_fasttext = np.array([text_to_avg_vector(text, fasttext_model) for text in test_df['text']])\n",
        "xgb_classifier_fasttext = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1, random_state=42, use_label_encoder=False)\n",
        "xgb_classifier_fasttext.fit(X_train_fasttext, y_train)\n",
        "y_test_pred_fasttext = xgb_classifier_fasttext.predict(X_test_fasttext)\n",
        "\n",
        "print(\"FastText with XGBoost Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_fasttext)}\")\n",
        "print(classification_report(y_test, y_test_pred_fasttext))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T21:50:58.406315Z",
          "iopub.execute_input": "2025-01-20T21:50:58.407216Z",
          "iopub.status.idle": "2025-01-20T21:52:01.426396Z",
          "shell.execute_reply.started": "2025-01-20T21:50:58.407182Z",
          "shell.execute_reply": "2025-01-20T21:52:01.425631Z"
        },
        "id": "WFvWot1Nm8C3",
        "outputId": "b38c67ae-aaea-436c-b8be-cf20f3a43ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "FastText with XGBoost Evaluation:\nAccuracy: 0.6085959008871215\n              precision    recall  f1-score   support\n\n           0       0.59      0.67      0.62       790\n           1       0.57      0.54      0.56       807\n           2       0.52      0.46      0.49       858\n           3       0.46      0.46      0.46       826\n           4       0.48      0.55      0.51       822\n           5       0.91      0.93      0.92       751\n           6       0.74      0.68      0.71       822\n           7       0.65      0.62      0.63       862\n\n    accuracy                           0.61      6538\n   macro avg       0.61      0.61      0.61      6538\nweighted avg       0.61      0.61      0.61      6538\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**"
      ],
      "metadata": {
        "id": "z57G1df9pua7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# keras Tokenizer"
      ],
      "metadata": {
        "id": "sZ4zT7yKp1D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import optuna\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
        "\n",
        "\n",
        "max_len = max(len(x) for x in X_train + X_test)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, lstm_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(lstm_units * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.attention(x)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.sum(weights * x, dim=1)\n",
        "        return context\n",
        "\n",
        "class ThreeLayerLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, lstm_units, output_dim, dropout_rate):\n",
        "        super(ThreeLayerLSTMWithAttention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm1 = nn.LSTM(embed_dim, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.attention = AttentionLayer(lstm_units)\n",
        "        self.fc = nn.Linear(lstm_units * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "output_dim = len(np.unique(y_train))\n",
        "\n",
        "# Optuna Objective Function\n",
        "def objective(trial):\n",
        "    # Пошук оптимальних гіперпараметрів\n",
        "    embed_dim = trial.suggest_categorical('embed_dim', [50, 100, 200])\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256, step=32)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    # Ініціалізація моделі\n",
        "    model = ThreeLayerLSTMWithAttention(len(tokenizer.word_index) + 1, embed_dim, lstm_units, output_dim, dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Тренування\n",
        "    def train_model(model, train_loader, optimizer, criterion, device, epochs=10):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss, correct = 0, 0\n",
        "            for texts, labels in train_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            accuracy = correct / len(train_loader.dataset)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    train_model(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Оцінка моделі\n",
        "    def evaluate_model(model, test_loader, device):\n",
        "        model.eval()\n",
        "        total_loss, correct = 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in test_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                preds = outputs.argmax(1).cpu().numpy()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "        print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    test_acc = evaluate_model(model, test_loader, device)\n",
        "    return test_acc\n",
        "\n",
        "# Optuna Study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "print(\"Best Parameters:\", study.best_params)\n",
        "print(\"Best Accuracy:\", study.best_value)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-25T23:52:35.561015Z",
          "iopub.execute_input": "2025-01-25T23:52:35.561333Z",
          "execution_failed": "2025-01-26T10:20:51.155Z"
        },
        "id": "TLwthpLzm8C3",
        "outputId": "92135664-3176-4792-db01-6b09df54c69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-25 23:52:38,354] A new study created in memory with name: no-name-5209f5ef-7ea0-4008-bf06-b11fee9e32fb\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10, Loss: 1.2855, Accuracy: 0.5353\nEpoch 2/10, Loss: 0.7400, Accuracy: 0.7564\nEpoch 3/10, Loss: 0.6098, Accuracy: 0.8010\nEpoch 4/10, Loss: 0.5136, Accuracy: 0.8364\nEpoch 5/10, Loss: 0.4645, Accuracy: 0.8509\nEpoch 6/10, Loss: 0.4647, Accuracy: 0.8512\nEpoch 7/10, Loss: 0.5136, Accuracy: 0.8341\nEpoch 8/10, Loss: 0.6071, Accuracy: 0.8021\nEpoch 9/10, Loss: 0.5787, Accuracy: 0.8096\nEpoch 10/10, Loss: 0.5249, Accuracy: 0.8283\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 00:27:01,018] Trial 0 finished with value: 0.7431936371979199 and parameters: {'embed_dim': 50, 'lstm_units': 192, 'dropout_rate': 0.2, 'learning_rate': 0.008646939722585803}. Best is trial 0 with value: 0.7431936371979199.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.8098, Test Accuracy: 0.7432\nEpoch 1/10, Loss: 1.8305, Accuracy: 0.2653\nEpoch 2/10, Loss: 1.0062, Accuracy: 0.6556\nEpoch 3/10, Loss: 0.7310, Accuracy: 0.7619\nEpoch 4/10, Loss: 0.6007, Accuracy: 0.8062\nEpoch 5/10, Loss: 0.5048, Accuracy: 0.8403\nEpoch 6/10, Loss: 0.4203, Accuracy: 0.8686\nEpoch 7/10, Loss: 0.3432, Accuracy: 0.8950\nEpoch 8/10, Loss: 0.2714, Accuracy: 0.9201\nEpoch 9/10, Loss: 0.2195, Accuracy: 0.9368\nEpoch 10/10, Loss: 0.1724, Accuracy: 0.9514\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 00:28:54,351] Trial 1 finished with value: 0.7479351483634139 and parameters: {'embed_dim': 100, 'lstm_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.0008628374171829393}. Best is trial 1 with value: 0.7479351483634139.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0499, Test Accuracy: 0.7479\nEpoch 1/10, Loss: 1.6125, Accuracy: 0.3791\nEpoch 2/10, Loss: 0.9016, Accuracy: 0.6988\nEpoch 3/10, Loss: 0.7144, Accuracy: 0.7662\nEpoch 4/10, Loss: 0.6134, Accuracy: 0.8026\nEpoch 5/10, Loss: 0.5234, Accuracy: 0.8329\nEpoch 6/10, Loss: 0.4366, Accuracy: 0.8617\nEpoch 7/10, Loss: 0.3520, Accuracy: 0.8918\nEpoch 8/10, Loss: 0.2707, Accuracy: 0.9170\nEpoch 9/10, Loss: 0.2106, Accuracy: 0.9358\nEpoch 10/10, Loss: 0.1560, Accuracy: 0.9548\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 00:50:41,687] Trial 2 finished with value: 0.7509941878250229 and parameters: {'embed_dim': 100, 'lstm_units': 160, 'dropout_rate': 0.4, 'learning_rate': 0.0007920465745204378}. Best is trial 2 with value: 0.7509941878250229.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0519, Test Accuracy: 0.7510\nEpoch 1/10, Loss: 1.5091, Accuracy: 0.4207\nEpoch 2/10, Loss: 0.7789, Accuracy: 0.7414\nEpoch 3/10, Loss: 0.6385, Accuracy: 0.7922\nEpoch 4/10, Loss: 0.5210, Accuracy: 0.8309\nEpoch 5/10, Loss: 0.4028, Accuracy: 0.8711\nEpoch 6/10, Loss: 0.2849, Accuracy: 0.9085\nEpoch 7/10, Loss: 0.1791, Accuracy: 0.9446\nEpoch 8/10, Loss: 0.1187, Accuracy: 0.9633\nEpoch 9/10, Loss: 0.0707, Accuracy: 0.9783\nEpoch 10/10, Loss: 0.0536, Accuracy: 0.9836\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:06:37,391] Trial 3 finished with value: 0.7561945549097583 and parameters: {'embed_dim': 100, 'lstm_units': 224, 'dropout_rate': 0.2, 'learning_rate': 0.001003806593726391}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.4267, Test Accuracy: 0.7562\nEpoch 1/10, Loss: 1.3707, Accuracy: 0.4805\nEpoch 2/10, Loss: 0.7099, Accuracy: 0.7709\nEpoch 3/10, Loss: 0.5353, Accuracy: 0.8314\nEpoch 4/10, Loss: 0.3922, Accuracy: 0.8769\nEpoch 5/10, Loss: 0.2715, Accuracy: 0.9175\nEpoch 6/10, Loss: 0.1860, Accuracy: 0.9439\nEpoch 7/10, Loss: 0.1251, Accuracy: 0.9631\nEpoch 8/10, Loss: 0.0908, Accuracy: 0.9732\nEpoch 9/10, Loss: 0.0766, Accuracy: 0.9769\nEpoch 10/10, Loss: 0.0668, Accuracy: 0.9809\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:08:30,540] Trial 4 finished with value: 0.7552768430712756 and parameters: {'embed_dim': 100, 'lstm_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.004243121380440028}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.1922, Test Accuracy: 0.7553\nEpoch 1/10, Loss: 1.8416, Accuracy: 0.2570\nEpoch 2/10, Loss: 1.0945, Accuracy: 0.6111\nEpoch 3/10, Loss: 0.7557, Accuracy: 0.7511\nEpoch 4/10, Loss: 0.6053, Accuracy: 0.8034\nEpoch 5/10, Loss: 0.5033, Accuracy: 0.8394\nEpoch 6/10, Loss: 0.4131, Accuracy: 0.8688\nEpoch 7/10, Loss: 0.3389, Accuracy: 0.8963\nEpoch 8/10, Loss: 0.2623, Accuracy: 0.9236\nEpoch 9/10, Loss: 0.2094, Accuracy: 0.9424\nEpoch 10/10, Loss: 0.1718, Accuracy: 0.9537\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:10:47,045] Trial 5 finished with value: 0.7436524931171612 and parameters: {'embed_dim': 200, 'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.0004509961195319397}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0861, Test Accuracy: 0.7437\nEpoch 1/10, Loss: 1.6840, Accuracy: 0.3327\nEpoch 2/10, Loss: 0.9533, Accuracy: 0.6642\nEpoch 3/10, Loss: 0.6856, Accuracy: 0.7785\nEpoch 4/10, Loss: 0.5546, Accuracy: 0.8244\nEpoch 5/10, Loss: 0.4469, Accuracy: 0.8622\nEpoch 6/10, Loss: 0.3577, Accuracy: 0.8920\nEpoch 7/10, Loss: 0.2733, Accuracy: 0.9185\nEpoch 8/10, Loss: 0.2044, Accuracy: 0.9411\nEpoch 9/10, Loss: 0.1609, Accuracy: 0.9555\nEpoch 10/10, Loss: 0.1231, Accuracy: 0.9661\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:12:40,183] Trial 6 finished with value: 0.7529825634750689 and parameters: {'embed_dim': 100, 'lstm_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.0012660207707198006}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0903, Test Accuracy: 0.7530\nEpoch 1/10, Loss: 1.8522, Accuracy: 0.2622\nEpoch 2/10, Loss: 1.0890, Accuracy: 0.6206\nEpoch 3/10, Loss: 0.8163, Accuracy: 0.7283\nEpoch 4/10, Loss: 0.7116, Accuracy: 0.7623\nEpoch 5/10, Loss: 0.6728, Accuracy: 0.7804\nEpoch 6/10, Loss: 0.5782, Accuracy: 0.8143\nEpoch 7/10, Loss: 0.5227, Accuracy: 0.8324\nEpoch 8/10, Loss: 0.4628, Accuracy: 0.8520\nEpoch 9/10, Loss: 0.4099, Accuracy: 0.8694\nEpoch 10/10, Loss: 0.3526, Accuracy: 0.8894\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:28:37,460] Trial 7 finished with value: 0.7488528602018966 and parameters: {'embed_dim': 100, 'lstm_units': 224, 'dropout_rate': 0.5, 'learning_rate': 0.00025579738459250107}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.8773, Test Accuracy: 0.7489\nEpoch 2/10, Loss: 1.0734, Accuracy: 0.6300\nEpoch 3/10, Loss: 0.8328, Accuracy: 0.7206\nEpoch 4/10, Loss: 0.7231, Accuracy: 0.7599\nEpoch 5/10, Loss: 0.6486, Accuracy: 0.7850\nEpoch 6/10, Loss: 0.5904, Accuracy: 0.8048\nEpoch 7/10, Loss: 0.5233, Accuracy: 0.8269\nEpoch 8/10, Loss: 0.4576, Accuracy: 0.8520\nEpoch 10/10, Loss: 0.3165, Accuracy: 0.8990\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 01:44:10,427] Trial 8 finished with value: 0.7488528602018966 and parameters: {'embed_dim': 50, 'lstm_units': 224, 'dropout_rate': 0.1, 'learning_rate': 0.00045178265641822226}. Best is trial 3 with value: 0.7561945549097583.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.9129, Test Accuracy: 0.7489\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "XtuO8FNKrLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import optuna\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "sentences = [text.split() for text in train_df['text']]  # Токенізація текстів\n",
        "\n",
        "# Тренування Word2Vec\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Перетворення текстів у послідовності\n",
        "word_index = {word: i + 1 for i, word in enumerate(word2vec_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def text_to_sequences(texts, word_index):\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        sequences.append([word_index.get(word, 0) for word in text.split()])\n",
        "    return sequences\n",
        "\n",
        "X_train = text_to_sequences(train_df['text'], word_index)\n",
        "X_test = text_to_sequences(test_df['text'], word_index)\n",
        "\n",
        "max_len = max(len(x) for x in X_train + X_test)\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Створення векторів для слів\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, lstm_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(lstm_units * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.attention(x)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.sum(weights * x, dim=1)\n",
        "        return context\n",
        "\n",
        "class ThreeLayerLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, lstm_units, output_dim, dropout_rate):\n",
        "        super(ThreeLayerLSTMWithAttention, self).__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm1 = nn.LSTM(embed_dim, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.attention = AttentionLayer(lstm_units)\n",
        "        self.fc = nn.Linear(lstm_units * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "output_dim = len(np.unique(y_train))\n",
        "\n",
        "# Optuna Objective Function\n",
        "def objective(trial):\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256, step=32)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    # Ініціалізація моделі\n",
        "    model = ThreeLayerLSTMWithAttention(embedding_matrix, lstm_units, output_dim, dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Тренування\n",
        "    def train_model(model, train_loader, optimizer, criterion, device, epochs=15):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss, correct = 0, 0\n",
        "            for texts, labels in train_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            accuracy = correct / len(train_loader.dataset)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    train_model(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Оцінка моделі\n",
        "    def evaluate_model(model, test_loader, device):\n",
        "        model.eval()\n",
        "        total_loss, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in test_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "        print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    test_acc = evaluate_model(model, test_loader, device)\n",
        "    return test_acc\n",
        "\n",
        "# Optuna Study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "print(\"Best Parameters:\", study.best_params)\n",
        "print(\"Best Accuracy:\", study.best_value)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-26T11:04:01.811968Z",
          "iopub.execute_input": "2025-01-26T11:04:01.812338Z",
          "execution_failed": "2025-01-26T20:24:41.854Z"
        },
        "id": "dbzgwilpm8C6",
        "outputId": "d8534c73-fdf9-4aea-a335-8ac97157b8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 11:04:11,649] A new study created in memory with name: no-name-0c0f6d95-93d5-43a1-a845-8f5a6350a222\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/15, Loss: 1.0784, Accuracy: 0.6177\nEpoch 2/15, Loss: 0.7106, Accuracy: 0.7724\nEpoch 3/15, Loss: 0.6416, Accuracy: 0.7934\nEpoch 4/15, Loss: 0.5935, Accuracy: 0.8088\nEpoch 5/15, Loss: 0.5486, Accuracy: 0.8230\nEpoch 6/15, Loss: 0.5048, Accuracy: 0.8384\nEpoch 7/15, Loss: 0.4456, Accuracy: 0.8568\nEpoch 8/15, Loss: 0.3822, Accuracy: 0.8766\nEpoch 9/15, Loss: 0.3231, Accuracy: 0.8963\nEpoch 10/15, Loss: 0.2578, Accuracy: 0.9168\nEpoch 11/15, Loss: 0.2029, Accuracy: 0.9355\nEpoch 12/15, Loss: 0.1627, Accuracy: 0.9491\nEpoch 13/15, Loss: 0.1289, Accuracy: 0.9579\nEpoch 14/15, Loss: 0.1158, Accuracy: 0.9638\nEpoch 15/15, Loss: 0.0888, Accuracy: 0.9722\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 11:13:34,782] Trial 0 finished with value: 0.7809727745487917 and parameters: {'lstm_units': 128, 'dropout_rate': 0.4, 'learning_rate': 0.0017795750765174576}. Best is trial 0 with value: 0.7809727745487917.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.1289, Test Accuracy: 0.7810\nEpoch 1/15, Loss: 1.3820, Accuracy: 0.4649\nEpoch 2/15, Loss: 0.7477, Accuracy: 0.7610\nEpoch 3/15, Loss: 0.6484, Accuracy: 0.7928\nEpoch 4/15, Loss: 0.6206, Accuracy: 0.7985\nEpoch 5/15, Loss: 0.5796, Accuracy: 0.8126\nEpoch 6/15, Loss: 0.5417, Accuracy: 0.8246\nEpoch 7/15, Loss: 0.5169, Accuracy: 0.8329\nEpoch 8/15, Loss: 0.4884, Accuracy: 0.8407\nEpoch 9/15, Loss: 0.4601, Accuracy: 0.8507\nEpoch 10/15, Loss: 0.4357, Accuracy: 0.8574\nEpoch 11/15, Loss: 0.4066, Accuracy: 0.8668\nEpoch 12/15, Loss: 0.3811, Accuracy: 0.8742\nEpoch 13/15, Loss: 0.3674, Accuracy: 0.8801\nEpoch 14/15, Loss: 0.4518, Accuracy: 0.8513\nEpoch 15/15, Loss: 0.4075, Accuracy: 0.8650\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 11:16:13,224] Trial 1 finished with value: 0.784796573875803 and parameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.006717123042575218}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.7134, Test Accuracy: 0.7848\nEpoch 1/15, Loss: 1.2407, Accuracy: 0.5453\nEpoch 2/15, Loss: 0.7718, Accuracy: 0.7435\nEpoch 3/15, Loss: 0.7055, Accuracy: 0.7684\nEpoch 4/15, Loss: 0.6609, Accuracy: 0.7823\nEpoch 5/15, Loss: 0.6316, Accuracy: 0.7900\nEpoch 6/15, Loss: 0.6024, Accuracy: 0.8011\nEpoch 7/15, Loss: 0.5749, Accuracy: 0.8101\nEpoch 8/15, Loss: 0.5473, Accuracy: 0.8184\nEpoch 9/15, Loss: 0.5187, Accuracy: 0.8297\nEpoch 10/15, Loss: 0.4868, Accuracy: 0.8390\nEpoch 11/15, Loss: 0.4518, Accuracy: 0.8499\nEpoch 12/15, Loss: 0.4143, Accuracy: 0.8633\nEpoch 13/15, Loss: 0.3734, Accuracy: 0.8757\nEpoch 14/15, Loss: 0.3317, Accuracy: 0.8898\nEpoch 15/15, Loss: 0.2882, Accuracy: 0.9053\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 12:08:34,631] Trial 2 finished with value: 0.7727133680024473 and parameters: {'lstm_units': 192, 'dropout_rate': 0.1, 'learning_rate': 0.0002823789073873486}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.8326, Test Accuracy: 0.7727\nEpoch 1/15, Loss: 1.0941, Accuracy: 0.6116\nEpoch 2/15, Loss: 0.7331, Accuracy: 0.7594\nEpoch 3/15, Loss: 0.6663, Accuracy: 0.7809\nEpoch 4/15, Loss: 0.6268, Accuracy: 0.7967\nEpoch 5/15, Loss: 0.5906, Accuracy: 0.8056\nEpoch 6/15, Loss: 0.5587, Accuracy: 0.8192\nEpoch 7/15, Loss: 0.5165, Accuracy: 0.8320\nEpoch 8/15, Loss: 0.4779, Accuracy: 0.8433\nEpoch 9/15, Loss: 0.4295, Accuracy: 0.8590\nEpoch 10/15, Loss: 0.3803, Accuracy: 0.8768\nEpoch 11/15, Loss: 0.3260, Accuracy: 0.8937\nEpoch 12/15, Loss: 0.2747, Accuracy: 0.9131\nEpoch 13/15, Loss: 0.2234, Accuracy: 0.9277\nEpoch 14/15, Loss: 0.1855, Accuracy: 0.9403\nEpoch 15/15, Loss: 0.1429, Accuracy: 0.9552\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 12:41:39,872] Trial 3 finished with value: 0.7754665035178954 and parameters: {'lstm_units': 160, 'dropout_rate': 0.4, 'learning_rate': 0.0008294484452742706}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0452, Test Accuracy: 0.7755\nEpoch 1/15, Loss: 1.2223, Accuracy: 0.5548\nEpoch 2/15, Loss: 0.7570, Accuracy: 0.7498\nEpoch 3/15, Loss: 0.6935, Accuracy: 0.7719\nEpoch 4/15, Loss: 0.6563, Accuracy: 0.7830\nEpoch 5/15, Loss: 0.6239, Accuracy: 0.7954\nEpoch 6/15, Loss: 0.5918, Accuracy: 0.8054\nEpoch 7/15, Loss: 0.5610, Accuracy: 0.8141\nEpoch 8/15, Loss: 0.5299, Accuracy: 0.8265\nEpoch 9/15, Loss: 0.4990, Accuracy: 0.8354\nEpoch 10/15, Loss: 0.4624, Accuracy: 0.8465\nEpoch 11/15, Loss: 0.4200, Accuracy: 0.8598\nEpoch 12/15, Loss: 0.3754, Accuracy: 0.8761\nEpoch 13/15, Loss: 0.3314, Accuracy: 0.8909\nEpoch 14/15, Loss: 0.2914, Accuracy: 0.9045\nEpoch 15/15, Loss: 0.2553, Accuracy: 0.9156\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 13:34:01,269] Trial 4 finished with value: 0.7633832976445396 and parameters: {'lstm_units': 192, 'dropout_rate': 0.1, 'learning_rate': 0.000335831239735828}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.8992, Test Accuracy: 0.7634\nEpoch 1/15, Loss: 1.1481, Accuracy: 0.5874\nEpoch 2/15, Loss: 0.7259, Accuracy: 0.7636\nEpoch 6/15, Loss: 0.5447, Accuracy: 0.8228\nEpoch 7/15, Loss: 0.5056, Accuracy: 0.8337\nEpoch 8/15, Loss: 0.4637, Accuracy: 0.8453\nEpoch 9/15, Loss: 0.4183, Accuracy: 0.8605\nEpoch 10/15, Loss: 0.3627, Accuracy: 0.8785\nEpoch 11/15, Loss: 0.3116, Accuracy: 0.8961\nEpoch 12/15, Loss: 0.2601, Accuracy: 0.9139\nEpoch 13/15, Loss: 0.2093, Accuracy: 0.9311\nEpoch 14/15, Loss: 0.1769, Accuracy: 0.9418\nEpoch 15/15, Loss: 0.1357, Accuracy: 0.9552\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 13:43:23,536] Trial 5 finished with value: 0.7739369837870909 and parameters: {'lstm_units': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0008910405638158334}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.0755, Test Accuracy: 0.7739\nEpoch 1/15, Loss: 1.2975, Accuracy: 0.5250\nEpoch 2/15, Loss: 0.7727, Accuracy: 0.7472\nEpoch 3/15, Loss: 0.6933, Accuracy: 0.7743\nEpoch 4/15, Loss: 0.6546, Accuracy: 0.7884\nEpoch 5/15, Loss: 0.6172, Accuracy: 0.7998\nEpoch 6/15, Loss: 0.5913, Accuracy: 0.8079\nEpoch 7/15, Loss: 0.5680, Accuracy: 0.8153\nEpoch 8/15, Loss: 0.5358, Accuracy: 0.8262\nEpoch 9/15, Loss: 0.5128, Accuracy: 0.8343\nEpoch 10/15, Loss: 0.4812, Accuracy: 0.8445\nEpoch 11/15, Loss: 0.4558, Accuracy: 0.8530\nEpoch 12/15, Loss: 0.4339, Accuracy: 0.8602\nEpoch 13/15, Loss: 0.3996, Accuracy: 0.8709\nEpoch 14/15, Loss: 0.3675, Accuracy: 0.8823\nEpoch 15/15, Loss: 0.3393, Accuracy: 0.8889\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 13:46:01,724] Trial 6 finished with value: 0.7705720403793209 and parameters: {'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.0006156299620968943}. Best is trial 1 with value: 0.784796573875803.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.7811, Test Accuracy: 0.7706\nEpoch 1/15, Loss: 1.4664, Accuracy: 0.4427\nEpoch 2/15, Loss: 0.8901, Accuracy: 0.7020\nEpoch 3/15, Loss: 0.7771, Accuracy: 0.7407\nEpoch 4/15, Loss: 0.7268, Accuracy: 0.7586\nEpoch 5/15, Loss: 0.6900, Accuracy: 0.7720\nEpoch 6/15, Loss: 0.6687, Accuracy: 0.7775\nEpoch 7/15, Loss: 0.6409, Accuracy: 0.7875\nEpoch 8/15, Loss: 0.6235, Accuracy: 0.7944\nEpoch 9/15, Loss: 0.6010, Accuracy: 0.8013\nEpoch 10/15, Loss: 0.5780, Accuracy: 0.8086\nEpoch 11/15, Loss: 0.5600, Accuracy: 0.8138\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "nAA7vLwTre70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import optuna\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Обчислення TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
        "X_test = vectorizer.transform(test_texts).toarray()\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Модель без шару Embedding\n",
        "class SimpleNNWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, lstm_units, output_dim, dropout_rate):\n",
        "        super(SimpleNNWithAttention, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.attention = nn.Linear(lstm_units * 2, 1)\n",
        "        self.fc = nn.Linear(lstm_units * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x.unsqueeze(1))\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        scores = self.attention(x)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.sum(weights * x, dim=1)\n",
        "        x = self.dropout(context)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(np.unique(y_train))\n",
        "\n",
        "# Optuna Objective Function\n",
        "def objective(trial):\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256, step=32)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    # Ініціалізація моделі\n",
        "    model = SimpleNNWithAttention(input_dim, lstm_units, output_dim, dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Тренування\n",
        "    def train_model(model, train_loader, optimizer, criterion, device, epochs=15):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss, correct = 0, 0\n",
        "            for texts, labels in train_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            accuracy = correct / len(train_loader.dataset)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    train_model(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Оцінка моделі\n",
        "    def evaluate_model(model, test_loader, device):\n",
        "        model.eval()\n",
        "        total_loss, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in test_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "        print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    test_acc = evaluate_model(model, test_loader, device)\n",
        "    return test_acc\n",
        "\n",
        "# Optuna Study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "print(\"Best Parameters:\", study.best_params)\n",
        "print(\"Best Accuracy:\", study.best_value)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-26T21:09:11.940701Z",
          "iopub.execute_input": "2025-01-26T21:09:11.941044Z",
          "iopub.status.idle": "2025-01-26T22:08:31.441962Z",
          "shell.execute_reply.started": "2025-01-26T21:09:11.941014Z",
          "shell.execute_reply": "2025-01-26T22:08:31.441119Z"
        },
        "id": "ldsvSPHSm8C7",
        "outputId": "fdb3cd09-2473-46dc-e277-e191e468dba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:09:17,719] A new study created in memory with name: no-name-146849a6-7aa9-4ae9-9f46-6e4ce1f08ddf\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/15, Loss: 0.9189, Accuracy: 0.6883\nEpoch 2/15, Loss: 0.4247, Accuracy: 0.8671\nEpoch 3/15, Loss: 0.2441, Accuracy: 0.9250\nEpoch 4/15, Loss: 0.1468, Accuracy: 0.9521\nEpoch 5/15, Loss: 0.0908, Accuracy: 0.9695\nEpoch 6/15, Loss: 0.0697, Accuracy: 0.9773\nEpoch 7/15, Loss: 0.0536, Accuracy: 0.9845\nEpoch 8/15, Loss: 0.0301, Accuracy: 0.9912\nEpoch 9/15, Loss: 0.0181, Accuracy: 0.9940\nEpoch 10/15, Loss: 0.0196, Accuracy: 0.9943\nEpoch 11/15, Loss: 0.0171, Accuracy: 0.9948\nEpoch 12/15, Loss: 0.0199, Accuracy: 0.9938\nEpoch 13/15, Loss: 0.0148, Accuracy: 0.9953\nEpoch 14/15, Loss: 0.0196, Accuracy: 0.9944\nEpoch 15/15, Loss: 0.0155, Accuracy: 0.9956\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:10:47,595] Trial 0 finished with value: 0.7485469562557358 and parameters: {'lstm_units': 256, 'dropout_rate': 0.5, 'learning_rate': 0.005604117449570261}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8516, Test Accuracy: 0.7485\nEpoch 1/15, Loss: 1.6059, Accuracy: 0.3709\nEpoch 2/15, Loss: 0.7328, Accuracy: 0.7504\nEpoch 3/15, Loss: 0.5071, Accuracy: 0.8368\nEpoch 4/15, Loss: 0.3746, Accuracy: 0.8842\nEpoch 5/15, Loss: 0.2768, Accuracy: 0.9172\nEpoch 6/15, Loss: 0.2027, Accuracy: 0.9424\nEpoch 7/15, Loss: 0.1477, Accuracy: 0.9608\nEpoch 8/15, Loss: 0.1042, Accuracy: 0.9752\nEpoch 9/15, Loss: 0.0704, Accuracy: 0.9842\nEpoch 10/15, Loss: 0.0489, Accuracy: 0.9901\nEpoch 11/15, Loss: 0.0342, Accuracy: 0.9935\nEpoch 12/15, Loss: 0.0242, Accuracy: 0.9961\nEpoch 13/15, Loss: 0.0176, Accuracy: 0.9972\nEpoch 14/15, Loss: 0.0134, Accuracy: 0.9977\nEpoch 15/15, Loss: 0.0102, Accuracy: 0.9984\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:12:17,521] Trial 1 finished with value: 0.7181095136127256 and parameters: {'lstm_units': 256, 'dropout_rate': 0.2, 'learning_rate': 0.0002125199371136607}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2458, Test Accuracy: 0.7181\nEpoch 1/15, Loss: 0.9170, Accuracy: 0.6868\nEpoch 2/15, Loss: 0.4206, Accuracy: 0.8692\nEpoch 3/15, Loss: 0.2373, Accuracy: 0.9277\nEpoch 4/15, Loss: 0.1401, Accuracy: 0.9559\nEpoch 5/15, Loss: 0.0870, Accuracy: 0.9721\nEpoch 6/15, Loss: 0.0671, Accuracy: 0.9787\nEpoch 7/15, Loss: 0.0422, Accuracy: 0.9873\nEpoch 8/15, Loss: 0.0237, Accuracy: 0.9920\nEpoch 9/15, Loss: 0.0214, Accuracy: 0.9932\nEpoch 10/15, Loss: 0.0176, Accuracy: 0.9942\nEpoch 11/15, Loss: 0.0188, Accuracy: 0.9937\nEpoch 12/15, Loss: 0.0151, Accuracy: 0.9953\nEpoch 13/15, Loss: 0.0184, Accuracy: 0.9946\nEpoch 14/15, Loss: 0.0127, Accuracy: 0.9960\nEpoch 15/15, Loss: 0.0101, Accuracy: 0.9970\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:13:42,901] Trial 2 finished with value: 0.7448761089018048 and parameters: {'lstm_units': 192, 'dropout_rate': 0.2, 'learning_rate': 0.005557324179522505}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8509, Test Accuracy: 0.7449\nEpoch 1/15, Loss: 2.0360, Accuracy: 0.1848\nEpoch 2/15, Loss: 1.5278, Accuracy: 0.4132\nEpoch 3/15, Loss: 0.9369, Accuracy: 0.6839\nEpoch 4/15, Loss: 0.6636, Accuracy: 0.7860\nEpoch 5/15, Loss: 0.5329, Accuracy: 0.8321\nEpoch 6/15, Loss: 0.4500, Accuracy: 0.8598\nEpoch 7/15, Loss: 0.3847, Accuracy: 0.8842\nEpoch 8/15, Loss: 0.3311, Accuracy: 0.9040\nEpoch 9/15, Loss: 0.2897, Accuracy: 0.9191\nEpoch 10/15, Loss: 0.2522, Accuracy: 0.9319\nEpoch 11/15, Loss: 0.2193, Accuracy: 0.9425\nEpoch 12/15, Loss: 0.1903, Accuracy: 0.9519\nEpoch 13/15, Loss: 0.1657, Accuracy: 0.9595\nEpoch 14/15, Loss: 0.1435, Accuracy: 0.9655\nEpoch 15/15, Loss: 0.1240, Accuracy: 0.9719\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:14:43,257] Trial 3 finished with value: 0.7416641174671154 and parameters: {'lstm_units': 128, 'dropout_rate': 0.4, 'learning_rate': 0.00011245506325579807}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.1705, Test Accuracy: 0.7417\nEpoch 1/15, Loss: 0.9354, Accuracy: 0.6761\nEpoch 2/15, Loss: 0.4286, Accuracy: 0.8651\nEpoch 3/15, Loss: 0.2481, Accuracy: 0.9255\nEpoch 4/15, Loss: 0.1463, Accuracy: 0.9542\nEpoch 5/15, Loss: 0.0890, Accuracy: 0.9718\nEpoch 6/15, Loss: 0.0586, Accuracy: 0.9811\nEpoch 7/15, Loss: 0.0393, Accuracy: 0.9865\nEpoch 8/15, Loss: 0.0317, Accuracy: 0.9903\nEpoch 9/15, Loss: 0.0211, Accuracy: 0.9927\nEpoch 10/15, Loss: 0.0220, Accuracy: 0.9932\nEpoch 11/15, Loss: 0.0204, Accuracy: 0.9934\nEpoch 12/15, Loss: 0.0156, Accuracy: 0.9950\nEpoch 13/15, Loss: 0.0100, Accuracy: 0.9969\nEpoch 14/15, Loss: 0.0130, Accuracy: 0.9954\nEpoch 15/15, Loss: 0.0144, Accuracy: 0.9958\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:15:37,666] Trial 4 finished with value: 0.7448761089018048 and parameters: {'lstm_units': 96, 'dropout_rate': 0.1, 'learning_rate': 0.005290311746412613}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8575, Test Accuracy: 0.7449\nEpoch 1/15, Loss: 1.3908, Accuracy: 0.4692\nEpoch 2/15, Loss: 0.5928, Accuracy: 0.8081\nEpoch 3/15, Loss: 0.3853, Accuracy: 0.8811\nEpoch 4/15, Loss: 0.2629, Accuracy: 0.9234\nEpoch 5/15, Loss: 0.1779, Accuracy: 0.9496\nEpoch 6/15, Loss: 0.1184, Accuracy: 0.9678\nEpoch 7/15, Loss: 0.0805, Accuracy: 0.9790\nEpoch 8/15, Loss: 0.0571, Accuracy: 0.9857\nEpoch 9/15, Loss: 0.0439, Accuracy: 0.9891\nEpoch 10/15, Loss: 0.0312, Accuracy: 0.9924\nEpoch 11/15, Loss: 0.0272, Accuracy: 0.9929\nEpoch 12/15, Loss: 0.0296, Accuracy: 0.9919\nEpoch 13/15, Loss: 0.0247, Accuracy: 0.9930\nEpoch 14/15, Loss: 0.0261, Accuracy: 0.9914\nEpoch 15/15, Loss: 0.0232, Accuracy: 0.9929\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:16:32,086] Trial 5 finished with value: 0.7272866319975527 and parameters: {'lstm_units': 96, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0006622922625834866}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0964, Test Accuracy: 0.7273\nEpoch 1/15, Loss: 1.0796, Accuracy: 0.6125\nEpoch 2/15, Loss: 0.4740, Accuracy: 0.8507\nEpoch 3/15, Loss: 0.3043, Accuracy: 0.9078\nEpoch 4/15, Loss: 0.2001, Accuracy: 0.9408\nEpoch 5/15, Loss: 0.1304, Accuracy: 0.9621\nEpoch 6/15, Loss: 0.0881, Accuracy: 0.9748\nEpoch 7/15, Loss: 0.0605, Accuracy: 0.9826\nEpoch 8/15, Loss: 0.0535, Accuracy: 0.9836\nEpoch 9/15, Loss: 0.0386, Accuracy: 0.9870\nEpoch 10/15, Loss: 0.0376, Accuracy: 0.9883\nEpoch 11/15, Loss: 0.0312, Accuracy: 0.9902\nEpoch 12/15, Loss: 0.0251, Accuracy: 0.9919\nEpoch 13/15, Loss: 0.0195, Accuracy: 0.9932\nEpoch 14/15, Loss: 0.0167, Accuracy: 0.9943\nEpoch 15/15, Loss: 0.0143, Accuracy: 0.9953\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:17:26,615] Trial 6 finished with value: 0.7298868155399205 and parameters: {'lstm_units': 96, 'dropout_rate': 0.2, 'learning_rate': 0.0015053221790182043}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.3656, Test Accuracy: 0.7299\nEpoch 1/15, Loss: 0.9162, Accuracy: 0.6918\nEpoch 2/15, Loss: 0.4282, Accuracy: 0.8676\nEpoch 3/15, Loss: 0.2369, Accuracy: 0.9267\nEpoch 4/15, Loss: 0.1424, Accuracy: 0.9551\nEpoch 5/15, Loss: 0.0804, Accuracy: 0.9731\nEpoch 6/15, Loss: 0.0574, Accuracy: 0.9804\nEpoch 7/15, Loss: 0.0490, Accuracy: 0.9852\nEpoch 8/15, Loss: 0.0220, Accuracy: 0.9928\nEpoch 9/15, Loss: 0.0214, Accuracy: 0.9936\nEpoch 10/15, Loss: 0.0131, Accuracy: 0.9957\nEpoch 11/15, Loss: 0.0227, Accuracy: 0.9931\nEpoch 12/15, Loss: 0.0190, Accuracy: 0.9946\nEpoch 13/15, Loss: 0.0151, Accuracy: 0.9954\nEpoch 14/15, Loss: 0.0113, Accuracy: 0.9969\nEpoch 15/15, Loss: 0.0069, Accuracy: 0.9979\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:18:48,754] Trial 7 finished with value: 0.7462526766595289 and parameters: {'lstm_units': 224, 'dropout_rate': 0.5, 'learning_rate': 0.006341272811321962}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0795, Test Accuracy: 0.7463\nEpoch 1/15, Loss: 0.9036, Accuracy: 0.6918\nEpoch 2/15, Loss: 0.4232, Accuracy: 0.8665\nEpoch 3/15, Loss: 0.2387, Accuracy: 0.9262\nEpoch 4/15, Loss: 0.1410, Accuracy: 0.9564\nEpoch 5/15, Loss: 0.0930, Accuracy: 0.9708\nEpoch 6/15, Loss: 0.0554, Accuracy: 0.9817\nEpoch 7/15, Loss: 0.0383, Accuracy: 0.9880\nEpoch 8/15, Loss: 0.0252, Accuracy: 0.9917\nEpoch 9/15, Loss: 0.0178, Accuracy: 0.9946\nEpoch 10/15, Loss: 0.0257, Accuracy: 0.9914\nEpoch 11/15, Loss: 0.0215, Accuracy: 0.9930\nEpoch 12/15, Loss: 0.0136, Accuracy: 0.9957\nEpoch 13/15, Loss: 0.0095, Accuracy: 0.9967\nEpoch 14/15, Loss: 0.0117, Accuracy: 0.9965\nEpoch 15/15, Loss: 0.0124, Accuracy: 0.9961\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:20:14,318] Trial 8 finished with value: 0.7433465891710003 and parameters: {'lstm_units': 192, 'dropout_rate': 0.1, 'learning_rate': 0.005600055507647418}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9404, Test Accuracy: 0.7433\nEpoch 1/15, Loss: 1.0479, Accuracy: 0.6172\nEpoch 2/15, Loss: 0.4552, Accuracy: 0.8559\nEpoch 3/15, Loss: 0.2711, Accuracy: 0.9163\nEpoch 4/15, Loss: 0.1711, Accuracy: 0.9491\nEpoch 5/15, Loss: 0.1200, Accuracy: 0.9635\nEpoch 6/15, Loss: 0.0859, Accuracy: 0.9729\nEpoch 7/15, Loss: 0.0699, Accuracy: 0.9772\nEpoch 8/15, Loss: 0.0564, Accuracy: 0.9807\nEpoch 9/15, Loss: 0.0464, Accuracy: 0.9842\nEpoch 10/15, Loss: 0.0368, Accuracy: 0.9876\nEpoch 11/15, Loss: 0.0330, Accuracy: 0.9882\nEpoch 12/15, Loss: 0.0272, Accuracy: 0.9909\nEpoch 13/15, Loss: 0.0244, Accuracy: 0.9916\nEpoch 14/15, Loss: 0.0177, Accuracy: 0.9943\nEpoch 15/15, Loss: 0.0187, Accuracy: 0.9932\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:21:08,826] Trial 9 finished with value: 0.7324869990822882 and parameters: {'lstm_units': 96, 'dropout_rate': 0.4, 'learning_rate': 0.0020328452526324294}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2049, Test Accuracy: 0.7325\nEpoch 1/15, Loss: 1.2039, Accuracy: 0.5449\nEpoch 2/15, Loss: 0.5069, Accuracy: 0.8337\nEpoch 3/15, Loss: 0.3054, Accuracy: 0.9064\nEpoch 4/15, Loss: 0.1947, Accuracy: 0.9415\nEpoch 5/15, Loss: 0.1263, Accuracy: 0.9622\nEpoch 6/15, Loss: 0.0850, Accuracy: 0.9749\nEpoch 7/15, Loss: 0.0653, Accuracy: 0.9793\nEpoch 8/15, Loss: 0.0493, Accuracy: 0.9856\nEpoch 9/15, Loss: 0.0411, Accuracy: 0.9871\nEpoch 10/15, Loss: 0.0354, Accuracy: 0.9892\nEpoch 11/15, Loss: 0.0344, Accuracy: 0.9892\nEpoch 12/15, Loss: 0.0291, Accuracy: 0.9902\nEpoch 13/15, Loss: 0.0208, Accuracy: 0.9937\nEpoch 14/15, Loss: 0.0240, Accuracy: 0.9925\nEpoch 15/15, Loss: 0.0234, Accuracy: 0.9922\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:22:38,629] Trial 10 finished with value: 0.7285102477821964 and parameters: {'lstm_units': 256, 'dropout_rate': 0.5, 'learning_rate': 0.0006012551254849993}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2615, Test Accuracy: 0.7285\nEpoch 1/15, Loss: 0.9218, Accuracy: 0.6901\nEpoch 2/15, Loss: 0.4223, Accuracy: 0.8680\nEpoch 3/15, Loss: 0.2154, Accuracy: 0.9314\nEpoch 4/15, Loss: 0.1168, Accuracy: 0.9628\nEpoch 5/15, Loss: 0.0714, Accuracy: 0.9794\nEpoch 6/15, Loss: 0.0468, Accuracy: 0.9863\nEpoch 7/15, Loss: 0.0386, Accuracy: 0.9881\nEpoch 8/15, Loss: 0.0296, Accuracy: 0.9915\nEpoch 9/15, Loss: 0.0280, Accuracy: 0.9917\nEpoch 10/15, Loss: 0.0206, Accuracy: 0.9945\nEpoch 11/15, Loss: 0.0185, Accuracy: 0.9951\nEpoch 12/15, Loss: 0.0154, Accuracy: 0.9959\nEpoch 13/15, Loss: 0.0122, Accuracy: 0.9963\nEpoch 14/15, Loss: 0.0115, Accuracy: 0.9971\nEpoch 15/15, Loss: 0.0163, Accuracy: 0.9958\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:24:01,142] Trial 11 finished with value: 0.7419700214132763 and parameters: {'lstm_units': 224, 'dropout_rate': 0.5, 'learning_rate': 0.009711073692802262}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0686, Test Accuracy: 0.7420\nEpoch 1/15, Loss: 0.9418, Accuracy: 0.6680\nEpoch 2/15, Loss: 0.4174, Accuracy: 0.8698\nEpoch 3/15, Loss: 0.2539, Accuracy: 0.9218\nEpoch 4/15, Loss: 0.1693, Accuracy: 0.9471\nEpoch 5/15, Loss: 0.1222, Accuracy: 0.9612\nEpoch 6/15, Loss: 0.0939, Accuracy: 0.9695\nEpoch 7/15, Loss: 0.0738, Accuracy: 0.9749\nEpoch 8/15, Loss: 0.0497, Accuracy: 0.9841\nEpoch 9/15, Loss: 0.0430, Accuracy: 0.9854\nEpoch 10/15, Loss: 0.0394, Accuracy: 0.9876\nEpoch 11/15, Loss: 0.0361, Accuracy: 0.9879\nEpoch 12/15, Loss: 0.0270, Accuracy: 0.9912\nEpoch 13/15, Loss: 0.0166, Accuracy: 0.9947\nEpoch 14/15, Loss: 0.0180, Accuracy: 0.9943\nEpoch 15/15, Loss: 0.0194, Accuracy: 0.9938\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:25:23,979] Trial 12 finished with value: 0.7390639339247477 and parameters: {'lstm_units': 224, 'dropout_rate': 0.5, 'learning_rate': 0.002597282209867328}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0383, Test Accuracy: 0.7391\nEpoch 1/15, Loss: 0.9381, Accuracy: 0.6827\nEpoch 2/15, Loss: 0.4172, Accuracy: 0.8712\nEpoch 3/15, Loss: 0.2163, Accuracy: 0.9341\nEpoch 4/15, Loss: 0.1098, Accuracy: 0.9657\nEpoch 5/15, Loss: 0.0715, Accuracy: 0.9772\nEpoch 6/15, Loss: 0.0452, Accuracy: 0.9859\nEpoch 7/15, Loss: 0.0248, Accuracy: 0.9926\nEpoch 8/15, Loss: 0.0251, Accuracy: 0.9924\nEpoch 9/15, Loss: 0.0255, Accuracy: 0.9925\nEpoch 10/15, Loss: 0.0213, Accuracy: 0.9933\nEpoch 11/15, Loss: 0.0186, Accuracy: 0.9946\nEpoch 12/15, Loss: 0.0221, Accuracy: 0.9937\nEpoch 13/15, Loss: 0.0157, Accuracy: 0.9958\nEpoch 14/15, Loss: 0.0139, Accuracy: 0.9958\nEpoch 15/15, Loss: 0.0126, Accuracy: 0.9965\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:26:34,186] Trial 13 finished with value: 0.7456408687672071 and parameters: {'lstm_units': 160, 'dropout_rate': 0.4, 'learning_rate': 0.009155423040985072}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8364, Test Accuracy: 0.7456\nEpoch 1/15, Loss: 0.9459, Accuracy: 0.6713\nEpoch 2/15, Loss: 0.4343, Accuracy: 0.8654\nEpoch 3/15, Loss: 0.2583, Accuracy: 0.9210\nEpoch 4/15, Loss: 0.1686, Accuracy: 0.9458\nEpoch 5/15, Loss: 0.1116, Accuracy: 0.9650\nEpoch 6/15, Loss: 0.0794, Accuracy: 0.9737\nEpoch 7/15, Loss: 0.0581, Accuracy: 0.9808\nEpoch 8/15, Loss: 0.0405, Accuracy: 0.9871\nEpoch 9/15, Loss: 0.0337, Accuracy: 0.9893\nEpoch 10/15, Loss: 0.0246, Accuracy: 0.9924\nEpoch 11/15, Loss: 0.0156, Accuracy: 0.9948\nEpoch 12/15, Loss: 0.0177, Accuracy: 0.9945\nEpoch 13/15, Loss: 0.0242, Accuracy: 0.9922\nEpoch 14/15, Loss: 0.0176, Accuracy: 0.9945\nEpoch 15/15, Loss: 0.0088, Accuracy: 0.9971\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:27:56,350] Trial 14 finished with value: 0.742581829305598 and parameters: {'lstm_units': 224, 'dropout_rate': 0.5, 'learning_rate': 0.0035206034310175795}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1943, Test Accuracy: 0.7426\nEpoch 1/15, Loss: 1.0556, Accuracy: 0.6164\nEpoch 2/15, Loss: 0.4446, Accuracy: 0.8584\nEpoch 3/15, Loss: 0.2691, Accuracy: 0.9159\nEpoch 4/15, Loss: 0.1686, Accuracy: 0.9479\nEpoch 5/15, Loss: 0.1111, Accuracy: 0.9649\nEpoch 6/15, Loss: 0.0816, Accuracy: 0.9737\nEpoch 7/15, Loss: 0.0667, Accuracy: 0.9776\nEpoch 8/15, Loss: 0.0534, Accuracy: 0.9816\nEpoch 9/15, Loss: 0.0519, Accuracy: 0.9820\nEpoch 10/15, Loss: 0.0423, Accuracy: 0.9857\nEpoch 11/15, Loss: 0.0305, Accuracy: 0.9891\nEpoch 12/15, Loss: 0.0242, Accuracy: 0.9920\nEpoch 13/15, Loss: 0.0234, Accuracy: 0.9922\nEpoch 14/15, Loss: 0.0261, Accuracy: 0.9908\nEpoch 15/15, Loss: 0.0264, Accuracy: 0.9916\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:29:26,159] Trial 15 finished with value: 0.7236157846436219 and parameters: {'lstm_units': 256, 'dropout_rate': 0.4, 'learning_rate': 0.0008756668205078437}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.3202, Test Accuracy: 0.7236\nEpoch 1/15, Loss: 1.3957, Accuracy: 0.4626\nEpoch 2/15, Loss: 0.5970, Accuracy: 0.8057\nEpoch 3/15, Loss: 0.3999, Accuracy: 0.8744\nEpoch 4/15, Loss: 0.2818, Accuracy: 0.9131\nEpoch 5/15, Loss: 0.1958, Accuracy: 0.9436\nEpoch 6/15, Loss: 0.1341, Accuracy: 0.9621\nEpoch 7/15, Loss: 0.0888, Accuracy: 0.9765\nEpoch 8/15, Loss: 0.0603, Accuracy: 0.9839\nEpoch 9/15, Loss: 0.0379, Accuracy: 0.9915\nEpoch 10/15, Loss: 0.0272, Accuracy: 0.9941\nEpoch 11/15, Loss: 0.0190, Accuracy: 0.9966\nEpoch 12/15, Loss: 0.0177, Accuracy: 0.9959\nEpoch 13/15, Loss: 0.0256, Accuracy: 0.9923\nEpoch 14/15, Loss: 0.0310, Accuracy: 0.9896\nEpoch 15/15, Loss: 0.0240, Accuracy: 0.9923\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:30:51,767] Trial 16 finished with value: 0.7234628326705415 and parameters: {'lstm_units': 192, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00037280766038541623}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2255, Test Accuracy: 0.7235\nEpoch 1/15, Loss: 1.0634, Accuracy: 0.6200\nEpoch 2/15, Loss: 0.4642, Accuracy: 0.8512\nEpoch 3/15, Loss: 0.2850, Accuracy: 0.9127\nEpoch 4/15, Loss: 0.1854, Accuracy: 0.9436\nEpoch 5/15, Loss: 0.1260, Accuracy: 0.9607\nEpoch 6/15, Loss: 0.0930, Accuracy: 0.9706\nEpoch 7/15, Loss: 0.0723, Accuracy: 0.9766\nEpoch 8/15, Loss: 0.0573, Accuracy: 0.9814\nEpoch 9/15, Loss: 0.0455, Accuracy: 0.9850\nEpoch 10/15, Loss: 0.0383, Accuracy: 0.9883\nEpoch 11/15, Loss: 0.0340, Accuracy: 0.9886\nEpoch 12/15, Loss: 0.0317, Accuracy: 0.9896\nEpoch 13/15, Loss: 0.0342, Accuracy: 0.9883\nEpoch 14/15, Loss: 0.0255, Accuracy: 0.9917\nEpoch 15/15, Loss: 0.0183, Accuracy: 0.9938\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:32:02,111] Trial 17 finished with value: 0.7334047109207709 and parameters: {'lstm_units': 160, 'dropout_rate': 0.5, 'learning_rate': 0.0013212579351898294}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1896, Test Accuracy: 0.7334\nEpoch 1/15, Loss: 0.9130, Accuracy: 0.6841\nEpoch 2/15, Loss: 0.4211, Accuracy: 0.8682\nEpoch 3/15, Loss: 0.2575, Accuracy: 0.9203\nEpoch 4/15, Loss: 0.1680, Accuracy: 0.9471\nEpoch 5/15, Loss: 0.1109, Accuracy: 0.9655\nEpoch 6/15, Loss: 0.0862, Accuracy: 0.9717\nEpoch 7/15, Loss: 0.0579, Accuracy: 0.9804\nEpoch 8/15, Loss: 0.0421, Accuracy: 0.9873\nEpoch 9/15, Loss: 0.0343, Accuracy: 0.9895\nEpoch 10/15, Loss: 0.0205, Accuracy: 0.9932\nEpoch 11/15, Loss: 0.0183, Accuracy: 0.9939\nEpoch 12/15, Loss: 0.0170, Accuracy: 0.9950\nEpoch 13/15, Loss: 0.0178, Accuracy: 0.9941\nEpoch 14/15, Loss: 0.0184, Accuracy: 0.9941\nEpoch 15/15, Loss: 0.0115, Accuracy: 0.9964\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:33:24,340] Trial 18 finished with value: 0.7382991740593454 and parameters: {'lstm_units': 224, 'dropout_rate': 0.4, 'learning_rate': 0.003649481338657693}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1184, Test Accuracy: 0.7383\nEpoch 1/15, Loss: 0.9374, Accuracy: 0.6745\nEpoch 2/15, Loss: 0.4360, Accuracy: 0.8621\nEpoch 3/15, Loss: 0.2602, Accuracy: 0.9210\nEpoch 4/15, Loss: 0.1698, Accuracy: 0.9467\nEpoch 5/15, Loss: 0.1192, Accuracy: 0.9611\nEpoch 6/15, Loss: 0.0836, Accuracy: 0.9731\nEpoch 7/15, Loss: 0.0550, Accuracy: 0.9825\nEpoch 8/15, Loss: 0.0413, Accuracy: 0.9865\nEpoch 9/15, Loss: 0.0383, Accuracy: 0.9878\nEpoch 10/15, Loss: 0.0308, Accuracy: 0.9900\nEpoch 11/15, Loss: 0.0166, Accuracy: 0.9946\nEpoch 12/15, Loss: 0.0176, Accuracy: 0.9950\nEpoch 13/15, Loss: 0.0164, Accuracy: 0.9946\nEpoch 14/15, Loss: 0.0198, Accuracy: 0.9935\nEpoch 15/15, Loss: 0.0141, Accuracy: 0.9955\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:34:53,876] Trial 19 finished with value: 0.744264301009483 and parameters: {'lstm_units': 256, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.003220372970292253}. Best is trial 0 with value: 0.7485469562557358.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9993, Test Accuracy: 0.7443\nEpoch 1/15, Loss: 0.9646, Accuracy: 0.6698\nEpoch 2/15, Loss: 0.4565, Accuracy: 0.8625\nEpoch 3/15, Loss: 0.2623, Accuracy: 0.9219\nEpoch 4/15, Loss: 0.1573, Accuracy: 0.9528\nEpoch 5/15, Loss: 0.0971, Accuracy: 0.9699\nEpoch 6/15, Loss: 0.0651, Accuracy: 0.9800\nEpoch 7/15, Loss: 0.0471, Accuracy: 0.9853\nEpoch 8/15, Loss: 0.0327, Accuracy: 0.9895\nEpoch 9/15, Loss: 0.0345, Accuracy: 0.9893\nEpoch 10/15, Loss: 0.0298, Accuracy: 0.9909\nEpoch 11/15, Loss: 0.0213, Accuracy: 0.9938\nEpoch 12/15, Loss: 0.0174, Accuracy: 0.9946\nEpoch 13/15, Loss: 0.0141, Accuracy: 0.9961\nEpoch 14/15, Loss: 0.0148, Accuracy: 0.9959\nEpoch 15/15, Loss: 0.0144, Accuracy: 0.9959\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:35:43,303] Trial 20 finished with value: 0.7500764759865403 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0069995939004768954}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9316, Test Accuracy: 0.7501\nEpoch 1/15, Loss: 0.9721, Accuracy: 0.6626\nEpoch 2/15, Loss: 0.4445, Accuracy: 0.8644\nEpoch 3/15, Loss: 0.2554, Accuracy: 0.9240\nEpoch 4/15, Loss: 0.1448, Accuracy: 0.9573\nEpoch 5/15, Loss: 0.0850, Accuracy: 0.9736\nEpoch 6/15, Loss: 0.0611, Accuracy: 0.9808\nEpoch 7/15, Loss: 0.0354, Accuracy: 0.9892\nEpoch 8/15, Loss: 0.0384, Accuracy: 0.9884\nEpoch 9/15, Loss: 0.0231, Accuracy: 0.9933\nEpoch 10/15, Loss: 0.0199, Accuracy: 0.9944\nEpoch 11/15, Loss: 0.0179, Accuracy: 0.9945\nEpoch 12/15, Loss: 0.0200, Accuracy: 0.9938\nEpoch 13/15, Loss: 0.0143, Accuracy: 0.9955\nEpoch 14/15, Loss: 0.0145, Accuracy: 0.9961\nEpoch 15/15, Loss: 0.0110, Accuracy: 0.9969\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:36:32,525] Trial 21 finished with value: 0.7404405016824717 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.007076564103594737}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8589, Test Accuracy: 0.7404\nEpoch 1/15, Loss: 1.0130, Accuracy: 0.6450\nEpoch 2/15, Loss: 0.4396, Accuracy: 0.8660\nEpoch 3/15, Loss: 0.2622, Accuracy: 0.9209\nEpoch 4/15, Loss: 0.1690, Accuracy: 0.9496\nEpoch 5/15, Loss: 0.1110, Accuracy: 0.9669\nEpoch 6/15, Loss: 0.0757, Accuracy: 0.9761\nEpoch 7/15, Loss: 0.0587, Accuracy: 0.9815\nEpoch 8/15, Loss: 0.0472, Accuracy: 0.9838\nEpoch 9/15, Loss: 0.0348, Accuracy: 0.9878\nEpoch 10/15, Loss: 0.0298, Accuracy: 0.9896\nEpoch 11/15, Loss: 0.0218, Accuracy: 0.9933\nEpoch 12/15, Loss: 0.0214, Accuracy: 0.9927\nEpoch 13/15, Loss: 0.0186, Accuracy: 0.9942\nEpoch 14/15, Loss: 0.0188, Accuracy: 0.9940\nEpoch 15/15, Loss: 0.0157, Accuracy: 0.9949\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:37:21,539] Trial 22 finished with value: 0.7464056286326094 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0040604508719355335}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0293, Test Accuracy: 0.7464\nEpoch 1/15, Loss: 1.0896, Accuracy: 0.6073\nEpoch 2/15, Loss: 0.4678, Accuracy: 0.8541\nEpoch 3/15, Loss: 0.2845, Accuracy: 0.9137\nEpoch 4/15, Loss: 0.1825, Accuracy: 0.9465\nEpoch 5/15, Loss: 0.1188, Accuracy: 0.9662\nEpoch 6/15, Loss: 0.0841, Accuracy: 0.9746\nEpoch 7/15, Loss: 0.0601, Accuracy: 0.9816\nEpoch 8/15, Loss: 0.0511, Accuracy: 0.9821\nEpoch 9/15, Loss: 0.0482, Accuracy: 0.9839\nEpoch 10/15, Loss: 0.0382, Accuracy: 0.9871\nEpoch 11/15, Loss: 0.0284, Accuracy: 0.9907\nEpoch 12/15, Loss: 0.0227, Accuracy: 0.9927\nEpoch 13/15, Loss: 0.0187, Accuracy: 0.9938\nEpoch 14/15, Loss: 0.0182, Accuracy: 0.9938\nEpoch 15/15, Loss: 0.0112, Accuracy: 0.9963\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:38:10,629] Trial 23 finished with value: 0.7306515754053228 and parameters: {'lstm_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.002091167129413628}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2875, Test Accuracy: 0.7307\nEpoch 1/15, Loss: 1.0041, Accuracy: 0.6453\nEpoch 2/15, Loss: 0.4576, Accuracy: 0.8566\nEpoch 3/15, Loss: 0.2757, Accuracy: 0.9186\nEpoch 4/15, Loss: 0.1707, Accuracy: 0.9497\nEpoch 5/15, Loss: 0.1104, Accuracy: 0.9675\nEpoch 6/15, Loss: 0.0835, Accuracy: 0.9725\nEpoch 7/15, Loss: 0.0611, Accuracy: 0.9801\nEpoch 8/15, Loss: 0.0449, Accuracy: 0.9853\nEpoch 9/15, Loss: 0.0326, Accuracy: 0.9895\nEpoch 10/15, Loss: 0.0326, Accuracy: 0.9898\nEpoch 11/15, Loss: 0.0219, Accuracy: 0.9932\nEpoch 12/15, Loss: 0.0188, Accuracy: 0.9942\nEpoch 13/15, Loss: 0.0143, Accuracy: 0.9958\nEpoch 14/15, Loss: 0.0119, Accuracy: 0.9965\nEpoch 15/15, Loss: 0.0165, Accuracy: 0.9951\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:38:59,691] Trial 24 finished with value: 0.745946772713368 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.004302257254285949}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9254, Test Accuracy: 0.7459\nEpoch 1/15, Loss: 0.9227, Accuracy: 0.6926\nEpoch 2/15, Loss: 0.4180, Accuracy: 0.8705\nEpoch 3/15, Loss: 0.2204, Accuracy: 0.9317\nEpoch 4/15, Loss: 0.1117, Accuracy: 0.9646\nEpoch 5/15, Loss: 0.0668, Accuracy: 0.9790\nEpoch 6/15, Loss: 0.0475, Accuracy: 0.9857\nEpoch 7/15, Loss: 0.0312, Accuracy: 0.9913\nEpoch 8/15, Loss: 0.0273, Accuracy: 0.9925\nEpoch 9/15, Loss: 0.0172, Accuracy: 0.9953\nEpoch 10/15, Loss: 0.0228, Accuracy: 0.9933\nEpoch 11/15, Loss: 0.0204, Accuracy: 0.9944\nEpoch 12/15, Loss: 0.0242, Accuracy: 0.9930\nEpoch 13/15, Loss: 0.0129, Accuracy: 0.9962\nEpoch 14/15, Loss: 0.0144, Accuracy: 0.9958\nEpoch 15/15, Loss: 0.0151, Accuracy: 0.9957\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:39:59,834] Trial 25 finished with value: 0.7356989905169776 and parameters: {'lstm_units': 128, 'dropout_rate': 0.4, 'learning_rate': 0.009984727734316332}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8835, Test Accuracy: 0.7357\nEpoch 1/15, Loss: 0.9798, Accuracy: 0.6538\nEpoch 2/15, Loss: 0.4332, Accuracy: 0.8607\nEpoch 3/15, Loss: 0.2588, Accuracy: 0.9226\nEpoch 4/15, Loss: 0.1751, Accuracy: 0.9462\nEpoch 5/15, Loss: 0.1244, Accuracy: 0.9615\nEpoch 6/15, Loss: 0.0928, Accuracy: 0.9705\nEpoch 7/15, Loss: 0.0714, Accuracy: 0.9766\nEpoch 8/15, Loss: 0.0522, Accuracy: 0.9837\nEpoch 9/15, Loss: 0.0440, Accuracy: 0.9848\nEpoch 10/15, Loss: 0.0384, Accuracy: 0.9870\nEpoch 11/15, Loss: 0.0295, Accuracy: 0.9902\nEpoch 12/15, Loss: 0.0215, Accuracy: 0.9931\nEpoch 13/15, Loss: 0.0217, Accuracy: 0.9938\nEpoch 14/15, Loss: 0.0249, Accuracy: 0.9920\nEpoch 15/15, Loss: 0.0178, Accuracy: 0.9943\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:40:59,863] Trial 26 finished with value: 0.7332517589476905 and parameters: {'lstm_units': 128, 'dropout_rate': 0.5, 'learning_rate': 0.0025575036009112652}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1899, Test Accuracy: 0.7333\nEpoch 1/15, Loss: 1.2077, Accuracy: 0.5559\nEpoch 2/15, Loss: 0.5036, Accuracy: 0.8417\nEpoch 3/15, Loss: 0.3182, Accuracy: 0.9057\nEpoch 4/15, Loss: 0.2080, Accuracy: 0.9412\nEpoch 5/15, Loss: 0.1423, Accuracy: 0.9608\nEpoch 6/15, Loss: 0.1021, Accuracy: 0.9708\nEpoch 7/15, Loss: 0.0707, Accuracy: 0.9800\nEpoch 8/15, Loss: 0.0536, Accuracy: 0.9838\nEpoch 9/15, Loss: 0.0469, Accuracy: 0.9860\nEpoch 10/15, Loss: 0.0414, Accuracy: 0.9868\nEpoch 11/15, Loss: 0.0328, Accuracy: 0.9895\nEpoch 12/15, Loss: 0.0267, Accuracy: 0.9911\nEpoch 13/15, Loss: 0.0217, Accuracy: 0.9926\nEpoch 14/15, Loss: 0.0211, Accuracy: 0.9934\nEpoch 15/15, Loss: 0.0191, Accuracy: 0.9935\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:41:49,593] Trial 27 finished with value: 0.7343224227592536 and parameters: {'lstm_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.001519714782035111}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1540, Test Accuracy: 0.7343\nEpoch 1/15, Loss: 0.9294, Accuracy: 0.6820\nEpoch 2/15, Loss: 0.4170, Accuracy: 0.8703\nEpoch 3/15, Loss: 0.2514, Accuracy: 0.9232\nEpoch 4/15, Loss: 0.1586, Accuracy: 0.9511\nEpoch 5/15, Loss: 0.1106, Accuracy: 0.9650\nEpoch 6/15, Loss: 0.0742, Accuracy: 0.9756\nEpoch 7/15, Loss: 0.0451, Accuracy: 0.9854\nEpoch 8/15, Loss: 0.0395, Accuracy: 0.9872\nEpoch 9/15, Loss: 0.0347, Accuracy: 0.9889\nEpoch 10/15, Loss: 0.0221, Accuracy: 0.9933\nEpoch 11/15, Loss: 0.0201, Accuracy: 0.9935\nEpoch 12/15, Loss: 0.0221, Accuracy: 0.9932\nEpoch 13/15, Loss: 0.0146, Accuracy: 0.9956\nEpoch 14/15, Loss: 0.0104, Accuracy: 0.9969\nEpoch 15/15, Loss: 0.0137, Accuracy: 0.9961\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:42:49,745] Trial 28 finished with value: 0.7396757418170694 and parameters: {'lstm_units': 128, 'dropout_rate': 0.5, 'learning_rate': 0.004388592092827149}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9442, Test Accuracy: 0.7397\nEpoch 1/15, Loss: 1.8763, Accuracy: 0.2467\nEpoch 2/15, Loss: 1.0985, Accuracy: 0.6152\nEpoch 3/15, Loss: 0.6404, Accuracy: 0.7915\nEpoch 4/15, Loss: 0.4801, Accuracy: 0.8493\nEpoch 5/15, Loss: 0.3817, Accuracy: 0.8857\nEpoch 6/15, Loss: 0.3085, Accuracy: 0.9095\nEpoch 7/15, Loss: 0.2493, Accuracy: 0.9292\nEpoch 8/15, Loss: 0.2012, Accuracy: 0.9468\nEpoch 9/15, Loss: 0.1626, Accuracy: 0.9596\nEpoch 10/15, Loss: 0.1292, Accuracy: 0.9690\nEpoch 11/15, Loss: 0.1020, Accuracy: 0.9778\nEpoch 12/15, Loss: 0.0810, Accuracy: 0.9825\nEpoch 13/15, Loss: 0.0646, Accuracy: 0.9868\nEpoch 14/15, Loss: 0.0491, Accuracy: 0.9914\nEpoch 15/15, Loss: 0.0378, Accuracy: 0.9943\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:43:43,956] Trial 29 finished with value: 0.7230039767513001 and parameters: {'lstm_units': 96, 'dropout_rate': 0.2, 'learning_rate': 0.0002202600489578813}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.6692, Test Accuracy: 0.7230\nEpoch 1/15, Loss: 0.9166, Accuracy: 0.6886\nEpoch 2/15, Loss: 0.4276, Accuracy: 0.8670\nEpoch 3/15, Loss: 0.2364, Accuracy: 0.9262\nEpoch 4/15, Loss: 0.1351, Accuracy: 0.9566\nEpoch 5/15, Loss: 0.0853, Accuracy: 0.9722\nEpoch 6/15, Loss: 0.0645, Accuracy: 0.9793\nEpoch 7/15, Loss: 0.0417, Accuracy: 0.9868\nEpoch 8/15, Loss: 0.0269, Accuracy: 0.9924\nEpoch 9/15, Loss: 0.0259, Accuracy: 0.9920\nEpoch 10/15, Loss: 0.0194, Accuracy: 0.9943\nEpoch 11/15, Loss: 0.0194, Accuracy: 0.9950\nEpoch 12/15, Loss: 0.0154, Accuracy: 0.9955\nEpoch 13/15, Loss: 0.0194, Accuracy: 0.9943\nEpoch 14/15, Loss: 0.0161, Accuracy: 0.9955\nEpoch 15/15, Loss: 0.0073, Accuracy: 0.9981\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:44:53,755] Trial 30 finished with value: 0.7457938207402875 and parameters: {'lstm_units': 160, 'dropout_rate': 0.5, 'learning_rate': 0.007521208795646186}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0865, Test Accuracy: 0.7458\nEpoch 1/15, Loss: 0.9362, Accuracy: 0.6797\nEpoch 2/15, Loss: 0.4353, Accuracy: 0.8652\nEpoch 3/15, Loss: 0.2469, Accuracy: 0.9220\nEpoch 4/15, Loss: 0.1375, Accuracy: 0.9553\nEpoch 5/15, Loss: 0.0847, Accuracy: 0.9723\nEpoch 6/15, Loss: 0.0611, Accuracy: 0.9811\nEpoch 7/15, Loss: 0.0341, Accuracy: 0.9901\nEpoch 8/15, Loss: 0.0268, Accuracy: 0.9926\nEpoch 9/15, Loss: 0.0224, Accuracy: 0.9933\nEpoch 10/15, Loss: 0.0171, Accuracy: 0.9949\nEpoch 11/15, Loss: 0.0156, Accuracy: 0.9953\nEpoch 12/15, Loss: 0.0180, Accuracy: 0.9947\nEpoch 13/15, Loss: 0.0154, Accuracy: 0.9958\nEpoch 14/15, Loss: 0.0140, Accuracy: 0.9961\nEpoch 15/15, Loss: 0.0136, Accuracy: 0.9962\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:46:23,408] Trial 31 finished with value: 0.7422759253594371 and parameters: {'lstm_units': 256, 'dropout_rate': 0.5, 'learning_rate': 0.006429870580204266}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.7999, Test Accuracy: 0.7423\nEpoch 1/15, Loss: 0.9208, Accuracy: 0.6840\nEpoch 2/15, Loss: 0.4300, Accuracy: 0.8671\nEpoch 3/15, Loss: 0.2515, Accuracy: 0.9230\nEpoch 4/15, Loss: 0.1567, Accuracy: 0.9501\nEpoch 5/15, Loss: 0.1025, Accuracy: 0.9675\nEpoch 6/15, Loss: 0.0723, Accuracy: 0.9768\nEpoch 7/15, Loss: 0.0465, Accuracy: 0.9854\nEpoch 8/15, Loss: 0.0386, Accuracy: 0.9880\nEpoch 9/15, Loss: 0.0304, Accuracy: 0.9905\nEpoch 10/15, Loss: 0.0173, Accuracy: 0.9947\nEpoch 11/15, Loss: 0.0179, Accuracy: 0.9949\nEpoch 12/15, Loss: 0.0206, Accuracy: 0.9935\nEpoch 13/15, Loss: 0.0188, Accuracy: 0.9944\nEpoch 14/15, Loss: 0.0168, Accuracy: 0.9951\nEpoch 15/15, Loss: 0.0125, Accuracy: 0.9963\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:47:49,182] Trial 32 finished with value: 0.7462526766595289 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.004838265994484141}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9982, Test Accuracy: 0.7463\nEpoch 1/15, Loss: 0.9085, Accuracy: 0.6945\nEpoch 2/15, Loss: 0.4190, Accuracy: 0.8701\nEpoch 3/15, Loss: 0.2280, Accuracy: 0.9285\nEpoch 4/15, Loss: 0.1344, Accuracy: 0.9577\nEpoch 5/15, Loss: 0.0811, Accuracy: 0.9732\nEpoch 6/15, Loss: 0.0597, Accuracy: 0.9800\nEpoch 7/15, Loss: 0.0368, Accuracy: 0.9877\nEpoch 8/15, Loss: 0.0285, Accuracy: 0.9911\nEpoch 9/15, Loss: 0.0221, Accuracy: 0.9933\nEpoch 10/15, Loss: 0.0182, Accuracy: 0.9948\nEpoch 11/15, Loss: 0.0116, Accuracy: 0.9965\nEpoch 12/15, Loss: 0.0156, Accuracy: 0.9954\nEpoch 13/15, Loss: 0.0129, Accuracy: 0.9961\nEpoch 14/15, Loss: 0.0109, Accuracy: 0.9968\nEpoch 15/15, Loss: 0.0149, Accuracy: 0.9955\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:49:11,164] Trial 33 finished with value: 0.7349342306515754 and parameters: {'lstm_units': 224, 'dropout_rate': 0.4, 'learning_rate': 0.006581767717372688}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9702, Test Accuracy: 0.7349\nEpoch 1/15, Loss: 1.0299, Accuracy: 0.6356\nEpoch 2/15, Loss: 0.4696, Accuracy: 0.8513\nEpoch 3/15, Loss: 0.2921, Accuracy: 0.9120\nEpoch 4/15, Loss: 0.1863, Accuracy: 0.9429\nEpoch 5/15, Loss: 0.1295, Accuracy: 0.9594\nEpoch 6/15, Loss: 0.0907, Accuracy: 0.9705\nEpoch 7/15, Loss: 0.0725, Accuracy: 0.9761\nEpoch 8/15, Loss: 0.0553, Accuracy: 0.9817\nEpoch 9/15, Loss: 0.0439, Accuracy: 0.9845\nEpoch 10/15, Loss: 0.0334, Accuracy: 0.9886\nEpoch 11/15, Loss: 0.0282, Accuracy: 0.9904\nEpoch 12/15, Loss: 0.0314, Accuracy: 0.9899\nEpoch 13/15, Loss: 0.0237, Accuracy: 0.9923\nEpoch 14/15, Loss: 0.0149, Accuracy: 0.9954\nEpoch 15/15, Loss: 0.0129, Accuracy: 0.9958\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:50:00,245] Trial 34 finished with value: 0.7419700214132763 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0030186398469548943}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.2268, Test Accuracy: 0.7420\nEpoch 1/15, Loss: 0.8967, Accuracy: 0.7011\nEpoch 2/15, Loss: 0.4203, Accuracy: 0.8674\nEpoch 3/15, Loss: 0.2319, Accuracy: 0.9269\nEpoch 4/15, Loss: 0.1272, Accuracy: 0.9587\nEpoch 5/15, Loss: 0.0826, Accuracy: 0.9725\nEpoch 6/15, Loss: 0.0510, Accuracy: 0.9842\nEpoch 7/15, Loss: 0.0331, Accuracy: 0.9896\nEpoch 8/15, Loss: 0.0297, Accuracy: 0.9916\nEpoch 9/15, Loss: 0.0183, Accuracy: 0.9947\nEpoch 10/15, Loss: 0.0179, Accuracy: 0.9942\nEpoch 11/15, Loss: 0.0163, Accuracy: 0.9950\nEpoch 12/15, Loss: 0.0201, Accuracy: 0.9937\nEpoch 13/15, Loss: 0.0120, Accuracy: 0.9967\nEpoch 14/15, Loss: 0.0068, Accuracy: 0.9979\nEpoch 15/15, Loss: 0.0178, Accuracy: 0.9950\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:51:29,770] Trial 35 finished with value: 0.7419700214132763 and parameters: {'lstm_units': 256, 'dropout_rate': 0.4, 'learning_rate': 0.007422921103368616}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8044, Test Accuracy: 0.7420\nEpoch 1/15, Loss: 1.9913, Accuracy: 0.1962\nEpoch 2/15, Loss: 1.4162, Accuracy: 0.4483\nEpoch 3/15, Loss: 0.9454, Accuracy: 0.6771\nEpoch 4/15, Loss: 0.6688, Accuracy: 0.7849\nEpoch 5/15, Loss: 0.5319, Accuracy: 0.8304\nEpoch 6/15, Loss: 0.4452, Accuracy: 0.8618\nEpoch 7/15, Loss: 0.3761, Accuracy: 0.8870\nEpoch 8/15, Loss: 0.3219, Accuracy: 0.9058\nEpoch 9/15, Loss: 0.2780, Accuracy: 0.9232\nEpoch 10/15, Loss: 0.2378, Accuracy: 0.9353\nEpoch 11/15, Loss: 0.2007, Accuracy: 0.9470\nEpoch 12/15, Loss: 0.1725, Accuracy: 0.9561\nEpoch 13/15, Loss: 0.1480, Accuracy: 0.9639\nEpoch 14/15, Loss: 0.1241, Accuracy: 0.9702\nEpoch 15/15, Loss: 0.1037, Accuracy: 0.9767\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:52:55,077] Trial 36 finished with value: 0.7376873661670236 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.00010688656028662769}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.2561, Test Accuracy: 0.7377\nEpoch 1/15, Loss: 0.9582, Accuracy: 0.6615\nEpoch 2/15, Loss: 0.4326, Accuracy: 0.8634\nEpoch 3/15, Loss: 0.2436, Accuracy: 0.9270\nEpoch 4/15, Loss: 0.1446, Accuracy: 0.9570\nEpoch 5/15, Loss: 0.0861, Accuracy: 0.9732\nEpoch 6/15, Loss: 0.0538, Accuracy: 0.9841\nEpoch 7/15, Loss: 0.0376, Accuracy: 0.9886\nEpoch 8/15, Loss: 0.0297, Accuracy: 0.9911\nEpoch 9/15, Loss: 0.0234, Accuracy: 0.9923\nEpoch 10/15, Loss: 0.0207, Accuracy: 0.9933\nEpoch 11/15, Loss: 0.0134, Accuracy: 0.9956\nEpoch 12/15, Loss: 0.0137, Accuracy: 0.9962\nEpoch 13/15, Loss: 0.0161, Accuracy: 0.9950\nEpoch 14/15, Loss: 0.0183, Accuracy: 0.9947\nEpoch 15/15, Loss: 0.0084, Accuracy: 0.9975\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:53:55,380] Trial 37 finished with value: 0.7448761089018048 and parameters: {'lstm_units': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0053551586199561295}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9886, Test Accuracy: 0.7449\nEpoch 1/15, Loss: 0.9566, Accuracy: 0.6620\nEpoch 2/15, Loss: 0.4268, Accuracy: 0.8649\nEpoch 3/15, Loss: 0.2509, Accuracy: 0.9233\nEpoch 4/15, Loss: 0.1554, Accuracy: 0.9512\nEpoch 5/15, Loss: 0.1124, Accuracy: 0.9629\nEpoch 6/15, Loss: 0.0736, Accuracy: 0.9758\nEpoch 7/15, Loss: 0.0529, Accuracy: 0.9828\nEpoch 8/15, Loss: 0.0426, Accuracy: 0.9855\nEpoch 9/15, Loss: 0.0331, Accuracy: 0.9894\nEpoch 10/15, Loss: 0.0216, Accuracy: 0.9933\nEpoch 11/15, Loss: 0.0215, Accuracy: 0.9935\nEpoch 12/15, Loss: 0.0200, Accuracy: 0.9940\nEpoch 13/15, Loss: 0.0160, Accuracy: 0.9952\nEpoch 14/15, Loss: 0.0154, Accuracy: 0.9953\nEpoch 15/15, Loss: 0.0161, Accuracy: 0.9946\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:54:49,752] Trial 38 finished with value: 0.7412052615478739 and parameters: {'lstm_units': 96, 'dropout_rate': 0.4, 'learning_rate': 0.004106297419253331}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9927, Test Accuracy: 0.7412\nEpoch 1/15, Loss: 0.9471, Accuracy: 0.6693\nEpoch 2/15, Loss: 0.4071, Accuracy: 0.8710\nEpoch 3/15, Loss: 0.2387, Accuracy: 0.9254\nEpoch 4/15, Loss: 0.1553, Accuracy: 0.9508\nEpoch 5/15, Loss: 0.1177, Accuracy: 0.9627\nEpoch 6/15, Loss: 0.0943, Accuracy: 0.9692\nEpoch 7/15, Loss: 0.0633, Accuracy: 0.9792\nEpoch 8/15, Loss: 0.0526, Accuracy: 0.9823\nEpoch 9/15, Loss: 0.0451, Accuracy: 0.9849\nEpoch 10/15, Loss: 0.0330, Accuracy: 0.9887\nEpoch 11/15, Loss: 0.0293, Accuracy: 0.9902\nEpoch 12/15, Loss: 0.0215, Accuracy: 0.9932\nEpoch 13/15, Loss: 0.0197, Accuracy: 0.9938\nEpoch 14/15, Loss: 0.0197, Accuracy: 0.9936\nEpoch 15/15, Loss: 0.0166, Accuracy: 0.9942\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:56:11,921] Trial 39 finished with value: 0.7358519424900581 and parameters: {'lstm_units': 224, 'dropout_rate': 0.1, 'learning_rate': 0.002042489169588128}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.1103, Test Accuracy: 0.7359\nEpoch 1/15, Loss: 0.9552, Accuracy: 0.6653\nEpoch 2/15, Loss: 0.4415, Accuracy: 0.8616\nEpoch 3/15, Loss: 0.2612, Accuracy: 0.9223\nEpoch 4/15, Loss: 0.1536, Accuracy: 0.9529\nEpoch 5/15, Loss: 0.0965, Accuracy: 0.9706\nEpoch 6/15, Loss: 0.0608, Accuracy: 0.9807\nEpoch 7/15, Loss: 0.0468, Accuracy: 0.9858\nEpoch 8/15, Loss: 0.0294, Accuracy: 0.9905\nEpoch 9/15, Loss: 0.0232, Accuracy: 0.9930\nEpoch 10/15, Loss: 0.0234, Accuracy: 0.9921\nEpoch 11/15, Loss: 0.0262, Accuracy: 0.9917\nEpoch 12/15, Loss: 0.0203, Accuracy: 0.9937\nEpoch 13/15, Loss: 0.0145, Accuracy: 0.9956\nEpoch 14/15, Loss: 0.0117, Accuracy: 0.9967\nEpoch 15/15, Loss: 0.0106, Accuracy: 0.9967\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:57:06,026] Trial 40 finished with value: 0.7434995411440808 and parameters: {'lstm_units': 96, 'dropout_rate': 0.5, 'learning_rate': 0.005601702551572966}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0275, Test Accuracy: 0.7435\nEpoch 1/15, Loss: 0.9307, Accuracy: 0.6770\nEpoch 2/15, Loss: 0.4317, Accuracy: 0.8652\nEpoch 3/15, Loss: 0.2500, Accuracy: 0.9226\nEpoch 4/15, Loss: 0.1614, Accuracy: 0.9492\nEpoch 5/15, Loss: 0.1044, Accuracy: 0.9659\nEpoch 6/15, Loss: 0.0671, Accuracy: 0.9780\nEpoch 7/15, Loss: 0.0450, Accuracy: 0.9852\nEpoch 8/15, Loss: 0.0313, Accuracy: 0.9898\nEpoch 9/15, Loss: 0.0268, Accuracy: 0.9914\nEpoch 10/15, Loss: 0.0270, Accuracy: 0.9916\nEpoch 11/15, Loss: 0.0150, Accuracy: 0.9954\nEpoch 12/15, Loss: 0.0170, Accuracy: 0.9950\nEpoch 13/15, Loss: 0.0111, Accuracy: 0.9967\nEpoch 14/15, Loss: 0.0166, Accuracy: 0.9951\nEpoch 15/15, Loss: 0.0147, Accuracy: 0.9958\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:58:31,495] Trial 41 finished with value: 0.7474762924441726 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.00476974759499074}. Best is trial 20 with value: 0.7500764759865403.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9440, Test Accuracy: 0.7475\nEpoch 1/15, Loss: 0.9179, Accuracy: 0.6905\nEpoch 2/15, Loss: 0.4219, Accuracy: 0.8703\nEpoch 3/15, Loss: 0.2255, Accuracy: 0.9295\nEpoch 4/15, Loss: 0.1189, Accuracy: 0.9621\nEpoch 5/15, Loss: 0.0816, Accuracy: 0.9740\nEpoch 6/15, Loss: 0.0474, Accuracy: 0.9850\nEpoch 7/15, Loss: 0.0286, Accuracy: 0.9914\nEpoch 8/15, Loss: 0.0332, Accuracy: 0.9901\nEpoch 9/15, Loss: 0.0247, Accuracy: 0.9924\nEpoch 10/15, Loss: 0.0169, Accuracy: 0.9953\nEpoch 11/15, Loss: 0.0153, Accuracy: 0.9953\nEpoch 12/15, Loss: 0.0182, Accuracy: 0.9948\nEpoch 13/15, Loss: 0.0185, Accuracy: 0.9949\nEpoch 14/15, Loss: 0.0098, Accuracy: 0.9970\nEpoch 15/15, Loss: 0.0104, Accuracy: 0.9972\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 21:59:56,805] Trial 42 finished with value: 0.7502294279596207 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.008268713053920792}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9735, Test Accuracy: 0.7502\nEpoch 1/15, Loss: 0.9269, Accuracy: 0.6870\nEpoch 2/15, Loss: 0.4243, Accuracy: 0.8681\nEpoch 3/15, Loss: 0.2249, Accuracy: 0.9304\nEpoch 4/15, Loss: 0.1268, Accuracy: 0.9595\nEpoch 5/15, Loss: 0.0827, Accuracy: 0.9722\nEpoch 6/15, Loss: 0.0520, Accuracy: 0.9830\nEpoch 7/15, Loss: 0.0310, Accuracy: 0.9904\nEpoch 8/15, Loss: 0.0284, Accuracy: 0.9910\nEpoch 9/15, Loss: 0.0222, Accuracy: 0.9938\nEpoch 10/15, Loss: 0.0127, Accuracy: 0.9967\nEpoch 11/15, Loss: 0.0201, Accuracy: 0.9945\nEpoch 12/15, Loss: 0.0159, Accuracy: 0.9954\nEpoch 13/15, Loss: 0.0211, Accuracy: 0.9939\nEpoch 14/15, Loss: 0.0141, Accuracy: 0.9966\nEpoch 15/15, Loss: 0.0088, Accuracy: 0.9977\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:01:06,514] Trial 43 finished with value: 0.7422759253594371 and parameters: {'lstm_units': 160, 'dropout_rate': 0.5, 'learning_rate': 0.008412711897208795}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.6664, Test Accuracy: 0.7423\nEpoch 1/15, Loss: 0.9252, Accuracy: 0.6820\nEpoch 2/15, Loss: 0.4274, Accuracy: 0.8693\nEpoch 3/15, Loss: 0.2431, Accuracy: 0.9259\nEpoch 4/15, Loss: 0.1451, Accuracy: 0.9537\nEpoch 5/15, Loss: 0.0899, Accuracy: 0.9708\nEpoch 6/15, Loss: 0.0648, Accuracy: 0.9788\nEpoch 7/15, Loss: 0.0458, Accuracy: 0.9862\nEpoch 8/15, Loss: 0.0291, Accuracy: 0.9904\nEpoch 9/15, Loss: 0.0281, Accuracy: 0.9920\nEpoch 10/15, Loss: 0.0220, Accuracy: 0.9932\nEpoch 11/15, Loss: 0.0174, Accuracy: 0.9950\nEpoch 12/15, Loss: 0.0158, Accuracy: 0.9952\nEpoch 13/15, Loss: 0.0138, Accuracy: 0.9961\nEpoch 14/15, Loss: 0.0137, Accuracy: 0.9958\nEpoch 15/15, Loss: 0.0105, Accuracy: 0.9967\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:02:31,858] Trial 44 finished with value: 0.7413582135209544 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.005434706333420459}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9791, Test Accuracy: 0.7414\nEpoch 1/15, Loss: 0.9154, Accuracy: 0.6897\nEpoch 2/15, Loss: 0.4324, Accuracy: 0.8656\nEpoch 3/15, Loss: 0.2357, Accuracy: 0.9270\nEpoch 4/15, Loss: 0.1386, Accuracy: 0.9563\nEpoch 5/15, Loss: 0.0801, Accuracy: 0.9754\nEpoch 6/15, Loss: 0.0482, Accuracy: 0.9853\nEpoch 7/15, Loss: 0.0367, Accuracy: 0.9888\nEpoch 8/15, Loss: 0.0289, Accuracy: 0.9913\nEpoch 9/15, Loss: 0.0227, Accuracy: 0.9932\nEpoch 10/15, Loss: 0.0215, Accuracy: 0.9941\nEpoch 11/15, Loss: 0.0170, Accuracy: 0.9946\nEpoch 12/15, Loss: 0.0208, Accuracy: 0.9941\nEpoch 13/15, Loss: 0.0185, Accuracy: 0.9945\nEpoch 14/15, Loss: 0.0150, Accuracy: 0.9956\nEpoch 15/15, Loss: 0.0115, Accuracy: 0.9966\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:03:41,465] Trial 45 finished with value: 0.7416641174671154 and parameters: {'lstm_units': 160, 'dropout_rate': 0.5, 'learning_rate': 0.008042460086870911}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.8383, Test Accuracy: 0.7417\nEpoch 1/15, Loss: 0.9373, Accuracy: 0.6722\nEpoch 2/15, Loss: 0.4130, Accuracy: 0.8711\nEpoch 3/15, Loss: 0.2466, Accuracy: 0.9244\nEpoch 4/15, Loss: 0.1718, Accuracy: 0.9462\nEpoch 5/15, Loss: 0.1224, Accuracy: 0.9593\nEpoch 6/15, Loss: 0.0843, Accuracy: 0.9730\nEpoch 7/15, Loss: 0.0617, Accuracy: 0.9802\nEpoch 8/15, Loss: 0.0525, Accuracy: 0.9820\nEpoch 9/15, Loss: 0.0389, Accuracy: 0.9874\nEpoch 10/15, Loss: 0.0356, Accuracy: 0.9880\nEpoch 11/15, Loss: 0.0269, Accuracy: 0.9913\nEpoch 12/15, Loss: 0.0260, Accuracy: 0.9922\nEpoch 13/15, Loss: 0.0164, Accuracy: 0.9950\nEpoch 14/15, Loss: 0.0209, Accuracy: 0.9933\nEpoch 15/15, Loss: 0.0196, Accuracy: 0.9934\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:05:06,793] Trial 46 finished with value: 0.7343224227592536 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.002692856904680879}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0895, Test Accuracy: 0.7343\nEpoch 1/15, Loss: 0.9458, Accuracy: 0.6669\nEpoch 2/15, Loss: 0.4183, Accuracy: 0.8689\nEpoch 3/15, Loss: 0.2474, Accuracy: 0.9248\nEpoch 4/15, Loss: 0.1587, Accuracy: 0.9511\nEpoch 5/15, Loss: 0.1108, Accuracy: 0.9642\nEpoch 6/15, Loss: 0.0781, Accuracy: 0.9751\nEpoch 7/15, Loss: 0.0458, Accuracy: 0.9853\nEpoch 8/15, Loss: 0.0351, Accuracy: 0.9884\nEpoch 9/15, Loss: 0.0309, Accuracy: 0.9895\nEpoch 10/15, Loss: 0.0288, Accuracy: 0.9910\nEpoch 11/15, Loss: 0.0249, Accuracy: 0.9921\nEpoch 12/15, Loss: 0.0153, Accuracy: 0.9954\nEpoch 13/15, Loss: 0.0123, Accuracy: 0.9964\nEpoch 14/15, Loss: 0.0101, Accuracy: 0.9970\nEpoch 15/15, Loss: 0.0136, Accuracy: 0.9959\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:06:16,575] Trial 47 finished with value: 0.7399816457632303 and parameters: {'lstm_units': 160, 'dropout_rate': 0.4, 'learning_rate': 0.003700233720396253}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.0541, Test Accuracy: 0.7400\nEpoch 1/15, Loss: 0.9235, Accuracy: 0.6837\nEpoch 2/15, Loss: 0.4293, Accuracy: 0.8672\nEpoch 3/15, Loss: 0.2530, Accuracy: 0.9224\nEpoch 4/15, Loss: 0.1585, Accuracy: 0.9507\nEpoch 5/15, Loss: 0.1102, Accuracy: 0.9642\nEpoch 6/15, Loss: 0.0715, Accuracy: 0.9776\nEpoch 7/15, Loss: 0.0514, Accuracy: 0.9845\nEpoch 8/15, Loss: 0.0344, Accuracy: 0.9893\nEpoch 9/15, Loss: 0.0288, Accuracy: 0.9907\nEpoch 10/15, Loss: 0.0248, Accuracy: 0.9925\nEpoch 11/15, Loss: 0.0199, Accuracy: 0.9941\nEpoch 12/15, Loss: 0.0113, Accuracy: 0.9961\nEpoch 13/15, Loss: 0.0098, Accuracy: 0.9971\nEpoch 14/15, Loss: 0.0190, Accuracy: 0.9948\nEpoch 15/15, Loss: 0.0207, Accuracy: 0.9939\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:07:41,988] Trial 48 finished with value: 0.7372285102477822 and parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.004782640484733326}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9186, Test Accuracy: 0.7372\nEpoch 1/15, Loss: 0.9536, Accuracy: 0.6714\nEpoch 2/15, Loss: 0.4455, Accuracy: 0.8642\nEpoch 3/15, Loss: 0.2495, Accuracy: 0.9262\nEpoch 4/15, Loss: 0.1334, Accuracy: 0.9612\nEpoch 5/15, Loss: 0.0785, Accuracy: 0.9778\nEpoch 6/15, Loss: 0.0471, Accuracy: 0.9863\nEpoch 7/15, Loss: 0.0392, Accuracy: 0.9883\nEpoch 8/15, Loss: 0.0326, Accuracy: 0.9905\nEpoch 9/15, Loss: 0.0281, Accuracy: 0.9915\nEpoch 10/15, Loss: 0.0220, Accuracy: 0.9937\nEpoch 11/15, Loss: 0.0138, Accuracy: 0.9964\nEpoch 12/15, Loss: 0.0117, Accuracy: 0.9970\nEpoch 13/15, Loss: 0.0199, Accuracy: 0.9941\nEpoch 14/15, Loss: 0.0188, Accuracy: 0.9950\nEpoch 15/15, Loss: 0.0138, Accuracy: 0.9963\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:08:31,437] Trial 49 finished with value: 0.7338635668400122 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.00872002137873422}. Best is trial 42 with value: 0.7502294279596207.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.9768, Test Accuracy: 0.7339\nBest Parameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'learning_rate': 0.008268713053920792}\nBest Accuracy: 0.7502294279596207\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "VrpJCxPJsW3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import FastText\n",
        "import optuna\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "sentences = [text.split() for text in train_df['text']]\n",
        "\n",
        "# Тренування FastText\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Перетворення текстів у послідовності\n",
        "word_index = {word: i + 1 for i, word in enumerate(fasttext_model.wv.index_to_key)}  # Індексація слів\n",
        "vocab_size = len(word_index) + 1  # Розмір словника з урахуванням паддінгів\n",
        "\n",
        "def text_to_sequences(texts, word_index):\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        sequences.append([word_index.get(word, 0) for word in text.split()])\n",
        "    return sequences\n",
        "\n",
        "X_train = text_to_sequences(train_df['text'], word_index)\n",
        "X_test = text_to_sequences(test_df['text'], word_index)\n",
        "max_len = max(len(x) for x in X_train + X_test)\n",
        "X_train = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in X_train], dtype=torch.long)\n",
        "X_test = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in X_test], dtype=torch.long)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = torch.tensor(label_encoder.fit_transform(train_df['label']), dtype=torch.long)\n",
        "y_test = torch.tensor(label_encoder.transform(test_df['label']), dtype=torch.long)\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Створення векторів для слів\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = fasttext_model.wv[word]\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, lstm_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(lstm_units * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.attention(x)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.sum(weights * x, dim=1)\n",
        "        return context\n",
        "\n",
        "class ThreeLayerLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, lstm_units, output_dim, dropout_rate):\n",
        "        super(ThreeLayerLSTMWithAttention, self).__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm1 = nn.LSTM(embed_dim, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(lstm_units * 2, lstm_units, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
        "        self.attention = AttentionLayer(lstm_units)\n",
        "        self.fc = nn.Linear(lstm_units * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "output_dim = len(torch.unique(y_train))\n",
        "\n",
        "# Optuna Objective Function\n",
        "def objective(trial):\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256, step=32)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    # Ініціалізація моделі\n",
        "    model = ThreeLayerLSTMWithAttention(embedding_matrix, lstm_units, output_dim, dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Тренування\n",
        "    def train_model(model, train_loader, optimizer, criterion, device, epochs=15):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss, correct = 0, 0\n",
        "            for texts, labels in train_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            accuracy = correct / len(train_loader.dataset)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    train_model(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Оцінка моделі\n",
        "    def evaluate_model(model, test_loader, device):\n",
        "        model.eval()\n",
        "        total_loss, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in test_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "        print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    test_acc = evaluate_model(model, test_loader, device)\n",
        "    return test_acc\n",
        "\n",
        "# Optuna Study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=25)\n",
        "print(\"Best Parameters:\", study.best_params)\n",
        "print(\"Best Accuracy:\", study.best_value)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-26T22:50:23.257341Z",
          "iopub.execute_input": "2025-01-26T22:50:23.257655Z",
          "execution_failed": "2025-01-27T00:56:56.035Z"
        },
        "id": "eYyyAegqm8C8",
        "outputId": "b4b3dc3c-8781-46ab-e969-d10ba8f5f238"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 22:51:05,962] A new study created in memory with name: no-name-5d86dc2b-0ca5-41fa-8af5-b09db8f259e1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/15, Loss: 1.1132, Accuracy: 0.6001\nEpoch 2/15, Loss: 0.7176, Accuracy: 0.7718\nEpoch 3/15, Loss: 0.6681, Accuracy: 0.7871\nEpoch 4/15, Loss: 0.6270, Accuracy: 0.7997\nEpoch 5/15, Loss: 0.6399, Accuracy: 0.7979\nEpoch 6/15, Loss: 0.5882, Accuracy: 0.8107\nEpoch 7/15, Loss: 0.5743, Accuracy: 0.8135\nEpoch 8/15, Loss: 0.5698, Accuracy: 0.8151\nEpoch 9/15, Loss: 0.5470, Accuracy: 0.8242\nEpoch 10/15, Loss: 0.5410, Accuracy: 0.8228\nEpoch 11/15, Loss: 0.5411, Accuracy: 0.8249\nEpoch 12/15, Loss: 0.5405, Accuracy: 0.8240\nEpoch 13/15, Loss: 0.5358, Accuracy: 0.8252\nEpoch 14/15, Loss: 0.6290, Accuracy: 0.7965\nEpoch 15/15, Loss: 0.5731, Accuracy: 0.8129\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 23:01:14,499] Trial 0 finished with value: 0.7936677883144693 and parameters: {'lstm_units': 128, 'dropout_rate': 0.4, 'learning_rate': 0.008120518921581686}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.6442, Test Accuracy: 0.7937\nEpoch 1/15, Loss: 1.0596, Accuracy: 0.6263\nEpoch 2/15, Loss: 0.6746, Accuracy: 0.7848\nEpoch 3/15, Loss: 0.6103, Accuracy: 0.8050\nEpoch 4/15, Loss: 0.5597, Accuracy: 0.8193\nEpoch 5/15, Loss: 0.5084, Accuracy: 0.8346\nEpoch 6/15, Loss: 0.4529, Accuracy: 0.8534\nEpoch 7/15, Loss: 0.4083, Accuracy: 0.8655\nEpoch 8/15, Loss: 0.3632, Accuracy: 0.8816\nEpoch 9/15, Loss: 0.3166, Accuracy: 0.8976\nEpoch 10/15, Loss: 0.2908, Accuracy: 0.9054\nEpoch 11/15, Loss: 0.2565, Accuracy: 0.9170\nEpoch 12/15, Loss: 0.2479, Accuracy: 0.9170\nEpoch 13/15, Loss: 0.2050, Accuracy: 0.9333\nEpoch 14/15, Loss: 0.1962, Accuracy: 0.9338\nEpoch 15/15, Loss: 0.1914, Accuracy: 0.9370\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 23:11:17,933] Trial 1 finished with value: 0.7774548791679413 and parameters: {'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 0.0039606706717626}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.8900, Test Accuracy: 0.7775\nEpoch 1/15, Loss: 1.6219, Accuracy: 0.3763\nEpoch 2/15, Loss: 1.0522, Accuracy: 0.6446\nEpoch 3/15, Loss: 0.8623, Accuracy: 0.7182\nEpoch 4/15, Loss: 0.7843, Accuracy: 0.7435\nEpoch 5/15, Loss: 0.7334, Accuracy: 0.7618\nEpoch 6/15, Loss: 0.7051, Accuracy: 0.7733\nEpoch 7/15, Loss: 0.6714, Accuracy: 0.7831\nEpoch 8/15, Loss: 0.6529, Accuracy: 0.7891\nEpoch 9/15, Loss: 0.6306, Accuracy: 0.7967\nEpoch 10/15, Loss: 0.6152, Accuracy: 0.8033\nEpoch 11/15, Loss: 0.5955, Accuracy: 0.8087\nEpoch 12/15, Loss: 0.5742, Accuracy: 0.8176\nEpoch 13/15, Loss: 0.5558, Accuracy: 0.8210\nEpoch 14/15, Loss: 0.5447, Accuracy: 0.8250\nEpoch 15/15, Loss: 0.5200, Accuracy: 0.8348\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 23:14:06,435] Trial 2 finished with value: 0.7695013765677577 and parameters: {'lstm_units': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0002792500401415998}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.7184, Test Accuracy: 0.7695\nEpoch 1/15, Loss: 1.7474, Accuracy: 0.3226\nEpoch 2/15, Loss: 1.2056, Accuracy: 0.5720\nEpoch 3/15, Loss: 1.0204, Accuracy: 0.6518\nEpoch 4/15, Loss: 0.8971, Accuracy: 0.7006\nEpoch 5/15, Loss: 0.8317, Accuracy: 0.7241\nEpoch 6/15, Loss: 0.7839, Accuracy: 0.7436\nEpoch 7/15, Loss: 0.7435, Accuracy: 0.7564\nEpoch 8/15, Loss: 0.7140, Accuracy: 0.7663\nEpoch 9/15, Loss: 0.6918, Accuracy: 0.7747\nEpoch 10/15, Loss: 0.6736, Accuracy: 0.7800\nEpoch 11/15, Loss: 0.6556, Accuracy: 0.7869\nEpoch 12/15, Loss: 0.6405, Accuracy: 0.7926\nEpoch 13/15, Loss: 0.6248, Accuracy: 0.7987\nEpoch 14/15, Loss: 0.6272, Accuracy: 0.7973\nEpoch 15/15, Loss: 0.5989, Accuracy: 0.8065\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 23:19:38,609] Trial 3 finished with value: 0.7725604160293668 and parameters: {'lstm_units': 96, 'dropout_rate': 0.5, 'learning_rate': 0.00010463632023636518}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.6933, Test Accuracy: 0.7726\nEpoch 1/15, Loss: 2.0701, Accuracy: 0.1808\nEpoch 2/15, Loss: 1.9762, Accuracy: 0.2154\nEpoch 3/15, Loss: 1.9591, Accuracy: 0.2267\nEpoch 4/15, Loss: 1.9362, Accuracy: 0.2339\nEpoch 5/15, Loss: 1.9245, Accuracy: 0.2433\nEpoch 6/15, Loss: 1.9189, Accuracy: 0.2460\nEpoch 7/15, Loss: 1.8886, Accuracy: 0.2596\nEpoch 8/15, Loss: 1.8731, Accuracy: 0.2669\nEpoch 9/15, Loss: 1.8982, Accuracy: 0.2625\nEpoch 10/15, Loss: 1.8676, Accuracy: 0.2712\nEpoch 11/15, Loss: 1.8651, Accuracy: 0.2741\nEpoch 12/15, Loss: 1.8125, Accuracy: 0.3117\nEpoch 13/15, Loss: 1.7564, Accuracy: 0.3306\nEpoch 14/15, Loss: 1.7482, Accuracy: 0.3372\nEpoch 15/15, Loss: 1.7430, Accuracy: 0.3390\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-26 23:52:03,507] Trial 4 finished with value: 0.35377791373508716 and parameters: {'lstm_units': 256, 'dropout_rate': 0.1, 'learning_rate': 0.0076468917352103495}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 1.7396, Test Accuracy: 0.3538\nEpoch 1/15, Loss: 1.7458, Accuracy: 0.2902\nEpoch 2/15, Loss: 0.8466, Accuracy: 0.7215\nEpoch 3/15, Loss: 0.6972, Accuracy: 0.7743\nEpoch 4/15, Loss: 0.6527, Accuracy: 0.7891\nEpoch 5/15, Loss: 0.6146, Accuracy: 0.8006\nEpoch 6/15, Loss: 0.6087, Accuracy: 0.8028\nEpoch 7/15, Loss: 0.5726, Accuracy: 0.8142\nEpoch 8/15, Loss: 0.5933, Accuracy: 0.8070\nEpoch 9/15, Loss: 0.5530, Accuracy: 0.8204\nEpoch 10/15, Loss: 0.5161, Accuracy: 0.8323\nEpoch 11/15, Loss: 0.4959, Accuracy: 0.8380\nEpoch 12/15, Loss: 0.4791, Accuracy: 0.8421\nEpoch 13/15, Loss: 0.4671, Accuracy: 0.8452\nEpoch 14/15, Loss: 0.4532, Accuracy: 0.8529\nEpoch 15/15, Loss: 0.4340, Accuracy: 0.8573\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-27 00:18:24,009] Trial 5 finished with value: 0.7921382685836648 and parameters: {'lstm_units': 224, 'dropout_rate': 0.1, 'learning_rate': 0.007397114131086175}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.6648, Test Accuracy: 0.7921\nEpoch 1/15, Loss: 1.0456, Accuracy: 0.6368\nEpoch 2/15, Loss: 0.7020, Accuracy: 0.7752\nEpoch 3/15, Loss: 0.6480, Accuracy: 0.7942\nEpoch 4/15, Loss: 0.5930, Accuracy: 0.8127\nEpoch 5/15, Loss: 0.5460, Accuracy: 0.8261\nEpoch 6/15, Loss: 0.4997, Accuracy: 0.8394\nEpoch 7/15, Loss: 0.4519, Accuracy: 0.8553\nEpoch 8/15, Loss: 0.4209, Accuracy: 0.8654\nEpoch 9/15, Loss: 0.3772, Accuracy: 0.8797\nEpoch 10/15, Loss: 0.3275, Accuracy: 0.8947\nEpoch 11/15, Loss: 0.2970, Accuracy: 0.9063\nEpoch 12/15, Loss: 0.2583, Accuracy: 0.9168\nEpoch 13/15, Loss: 0.2345, Accuracy: 0.9238\nEpoch 14/15, Loss: 0.2151, Accuracy: 0.9321\nEpoch 15/15, Loss: 0.1846, Accuracy: 0.9398\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-01-27 00:23:58,756] Trial 6 finished with value: 0.7792903028449067 and parameters: {'lstm_units': 96, 'dropout_rate': 0.5, 'learning_rate': 0.003415265352030092}. Best is trial 0 with value: 0.7936677883144693.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 0.9166, Test Accuracy: 0.7793\nEpoch 1/15, Loss: 1.3624, Accuracy: 0.4901\nEpoch 2/15, Loss: 0.8202, Accuracy: 0.7236\nEpoch 3/15, Loss: 0.7412, Accuracy: 0.7524\nEpoch 4/15, Loss: 0.6913, Accuracy: 0.7686\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT**"
      ],
      "metadata": {
        "id": "H7Up3_i5swUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tqdm import tqdm\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def tokenize_data(texts, tokenizer, max_len):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "\n",
        "max_len = 256\n",
        "train_encodings = tokenize_data(train_df['text'].tolist(), tokenizer, max_len)\n",
        "test_encodings = tokenize_data(test_df['text'].tolist(), tokenizer, max_len)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['label'])\n",
        "y_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset = TextDataset(train_encodings, y_train)\n",
        "test_dataset = TextDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "#pre-trained BERT\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_)).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 7\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "def train_model(model, train_loader, optimizer, scheduler, device, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (predictions == batch['labels']).sum().item()\n",
        "            total_predictions += batch['labels'].size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "train_model(model, train_loader, optimizer, scheduler, device, epochs)\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "y_test_pred, y_test_labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "print(\"BERT Transformer Model Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, y_test_pred)}\")\n",
        "print(classification_report(y_test_labels, y_test_pred))\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T13:08:16.764496Z",
          "iopub.execute_input": "2025-01-27T13:08:16.765351Z",
          "iopub.status.idle": "2025-01-27T15:27:52.996296Z",
          "shell.execute_reply.started": "2025-01-27T13:08:16.765304Z",
          "shell.execute_reply": "2025-01-27T15:27:52.995443Z"
        },
        "id": "KsQxnmlEm8C9",
        "outputId": "bf16edee-62b0-4459-8528-6f2b891083e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/7: 100%|██████████| 1635/1635 [19:34<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/7 - Loss: 0.7709, Accuracy: 0.7505\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/7: 100%|██████████| 1635/1635 [19:35<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/7 - Loss: 0.4871, Accuracy: 0.8454\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/7: 100%|██████████| 1635/1635 [19:35<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/7 - Loss: 0.3413, Accuracy: 0.8906\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/7: 100%|██████████| 1635/1635 [19:34<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/7 - Loss: 0.2073, Accuracy: 0.9370\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/7: 100%|██████████| 1635/1635 [19:34<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/7 - Loss: 0.1197, Accuracy: 0.9667\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/7: 100%|██████████| 1635/1635 [19:32<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/7 - Loss: 0.0703, Accuracy: 0.9820\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/7: 100%|██████████| 1635/1635 [19:32<00:00,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/7 - Loss: 0.0442, Accuracy: 0.9896\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Evaluating: 100%|██████████| 409/409 [01:38<00:00,  4.14it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "BERT Transformer Model Evaluation:\nAccuracy: 0.8135515448149281\n              precision    recall  f1-score   support\n\n           0       0.88      0.84      0.85       790\n           1       0.75      0.79      0.77       807\n           2       0.79      0.76      0.78       858\n           3       0.76      0.72      0.74       826\n           4       0.67      0.74      0.71       822\n           5       0.99      0.97      0.98       751\n           6       0.87      0.86      0.87       822\n           7       0.83      0.84      0.84       862\n\n    accuracy                           0.81      6538\n   macro avg       0.82      0.82      0.82      6538\nweighted avg       0.82      0.81      0.81      6538\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}