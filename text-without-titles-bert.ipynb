{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T17:26:08.570557Z",
     "iopub.status.busy": "2024-12-22T17:26:08.570286Z",
     "iopub.status.idle": "2024-12-22T17:26:13.670786Z",
     "shell.execute_reply": "2024-12-22T17:26:13.670051Z",
     "shell.execute_reply.started": "2024-12-22T17:26:08.570530Z"
    },
    "id": "obOY0g3rEay3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:26:17.185783Z",
     "iopub.status.busy": "2024-12-22T17:26:17.184985Z",
     "iopub.status.idle": "2024-12-22T17:26:23.500290Z",
     "shell.execute_reply": "2024-12-22T17:26:23.499567Z",
     "shell.execute_reply.started": "2024-12-22T17:26:17.185750Z"
    },
    "id": "y7w5sH0ZEay8",
    "outputId": "5e7d3d34-1817-40f5-d749-78b0338dfaa8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:26:32.150141Z",
     "iopub.status.busy": "2024-12-22T17:26:32.149043Z",
     "iopub.status.idle": "2024-12-22T17:26:44.590572Z",
     "shell.execute_reply": "2024-12-22T17:26:44.589598Z",
     "shell.execute_reply.started": "2024-12-22T17:26:32.150104Z"
    },
    "id": "IyRdBSgDEazB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 55)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ce2b975710134899b108de0caf1ae339",
      "5004e05a49f64f0a9e953b509230ead4",
      "6996ce1abdb84f3fa702bae4db6007d2",
      "50bed933cdc345349e771754e782d045",
      "1a961d16f1834af49e61a3f80c9b0113",
      "32479126acbb49af9344bc3956224bae",
      "fb63166f86f74f0a95536a41dc09fd56"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-22T17:29:12.897788Z",
     "iopub.status.busy": "2024-12-22T17:29:12.896516Z",
     "iopub.status.idle": "2024-12-22T17:29:46.502056Z",
     "shell.execute_reply": "2024-12-22T17:29:46.501149Z",
     "shell.execute_reply.started": "2024-12-22T17:29:12.897754Z"
    },
    "id": "IaEWCOgaEazC",
    "outputId": "f170a8f5-4680-4a75-b378-442a04aaf551",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2b975710134899b108de0caf1ae339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/755 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5004e05a49f64f0a9e953b509230ead4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6996ce1abdb84f3fa702bae4db6007d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bed933cdc345349e771754e782d045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a961d16f1834af49e61a3f80c9b0113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32479126acbb49af9344bc3956224bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb63166f86f74f0a95536a41dc09fd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title  \\\n",
      "0                              fantasizing about your fp   \n",
      "1      this is a support subreddit for people with a ...   \n",
      "2                        scared of my psychotic symptoms   \n",
      "4      i feel like a sick animal that needs to be put...   \n",
      "5      is constantly checking ocd subreddits a compul...   \n",
      "...                                                  ...   \n",
      "42108  16m suicidal, lonely, need to feel loved befor...   \n",
      "42109                               got asked on a date!   \n",
      "42110         i just found out my friend killed herself.   \n",
      "42111  family letter detailing the conditions and lib...   \n",
      "42112                     i'm very proud of myself today   \n",
      "\n",
      "                                               main_text  label  \n",
      "0      do you do it? what do you fantasize about? : w...      3  \n",
      "1      it's not for posting how infuriating the uneve...      6  \n",
      "2      i'm trying to keep reminding myself that what ...      2  \n",
      "4      i had a cat for 18 years. at the end of her li...      4  \n",
      "5      i kid you not, when i have the thoughts, i’m c...      6  \n",
      "...                                                  ...    ...  \n",
      "42108  don't really know why i'm making this post, i ...      4  \n",
      "42109  and by a girl i already liked. pretty excited ...      1  \n",
      "42110  i had been trying to get her through this suic...      3  \n",
      "42111  this post is in relation to a post discussing ...      5  \n",
      "42112  i had an exam today and my anxiety started kic...      3  \n",
      "\n",
      "[38469 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset('mavinsao/reddit-mental-illness-82')\n",
    "df = dataset['train'].to_pandas()\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "keywords = [\n",
    "    \"think i have\", \"i think i have\", \"i think it might be\", \"i think i could have\",\n",
    "    \"might have\", \"i might have\", \"might be\", \"feel like i have\", \"i feel like i have\",\n",
    "    \"feels like i have\", \"self-diagnose\", \"self-diagnosed\", \"i've self-diagnosed\",\n",
    "    \"unsure if i have\", \"i'm unsure if\", \"unsure if this is\", \"wonder if i have\",\n",
    "    \"i wonder if i have\", \"wonder if it's\", \"symptoms of\", \"i have symptoms of\",\n",
    "    \"experiencing symptoms of\", \"suspect i have\", \"i suspect i have\", \"i suspect it's\",\n",
    "    \"probably have\", \"i probably have\", \"i think i probably have\", \"could be\", \"it could be\",\n",
    "    \"seems like i have\", \"it seems like i have\", \"it seems like\", \"not diagnosed but\",\n",
    "    \"i am not diagnosed but\", \"i haven't been diagnosed but\"\n",
    "]\n",
    "\n",
    "df = df[~df['text'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "# Розбиття тексту на заголовок і основний текст\n",
    "def split_text(row):\n",
    "    if ':' in row:\n",
    "        parts = row.split(':', 1)\n",
    "        return parts[0].strip(), parts[1].strip()\n",
    "    return None, row.strip()\n",
    "\n",
    "df[['title', 'main_text']] = df['text'].apply(split_text).apply(pd.Series)\n",
    "\n",
    "\n",
    "print(df[['title', 'main_text', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:35:23.067594Z",
     "iopub.status.busy": "2024-12-22T17:35:23.066026Z",
     "iopub.status.idle": "2024-12-22T17:47:05.739942Z",
     "shell.execute_reply": "2024-12-22T17:47:05.739008Z",
     "shell.execute_reply.started": "2024-12-22T17:35:23.067541Z"
    },
    "id": "1TKiSWtNEazE",
    "outputId": "911b2898-1c2e-4e6a-dfba-36a40732753a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label  \\\n",
      "0      fantasizing about your fp: do you do it? what ...      3   \n",
      "1      this is a support subreddit for people with a ...      6   \n",
      "2      scared of my psychotic symptoms : i'm trying t...      2   \n",
      "4      i feel like a sick animal that needs to be put...      4   \n",
      "5      is constantly checking ocd subreddits a compul...      6   \n",
      "...                                                  ...    ...   \n",
      "42108  16m suicidal, lonely, need to feel loved befor...      4   \n",
      "42109  got asked on a date! : and by a girl i already...      1   \n",
      "42110  i just found out my friend killed herself. : i...      3   \n",
      "42111  family letter detailing the conditions and lib...      5   \n",
      "42112  i'm very proud of myself today : i had an exam...      3   \n",
      "\n",
      "                                                   title  \\\n",
      "0                                           fantasize fp   \n",
      "1                support subreddit people mental illness   \n",
      "2                               scared psychotic symptom   \n",
      "4                         feel like sick animal need put   \n",
      "5              constantly check ocd subreddit compulsion   \n",
      "...                                                  ...   \n",
      "42108  suicidal lonely need feel loved end tonight ne...   \n",
      "42109                                       get ask date   \n",
      "42110                                   find friend kill   \n",
      "42111  family letter condition liberation concentrati...   \n",
      "42112                                    I m proud today   \n",
      "\n",
      "                                               main_text  \n",
      "0      fantasize fp attract fp romantic way fantasize...  \n",
      "1      post infuriate uneven floor tile satisfy candy...  \n",
      "2      I m try keep remind I m think believe be not r...  \n",
      "4      cat year end life lose mental faculty lethargi...  \n",
      "5      kid thought I m constantly check ocd subreddit...  \n",
      "...                                                  ...  \n",
      "42108  do not really know I m make post do not want a...  \n",
      "42109  girl already like pretty excited time seem mis...  \n",
      "42110  try get suicidality get medicare would not let...  \n",
      "42111  post relation post discuss holocaust another s...  \n",
      "42112  exam today anxiety start kick last night speak...  \n",
      "\n",
      "[38469 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Функція очищення тексту\n",
    "def clean_text(text):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    text = text.str.replace(\"\\n\", '', regex=True)\n",
    "    text = text.str.replace('\\d', '', regex=True)\n",
    "    text = text.str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "    text = text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
    "    text = text.str.replace(r'<.*?>+', '', regex=True)\n",
    "    text = text.str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
    "    return text\n",
    "\n",
    "# Функція видалення стоп-слів\n",
    "# custom_stopwords = {'ocd', 'anxiety', 'adhd', 'ptsd', 'bpd', 'depression', 'bipolar'}\n",
    "custom_stopwords = {}\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(custom_stopwords)\n",
    "    text = text.apply(lambda x: \" \".join(word for word in str(x).split() if word.lower() not in stop_words))\n",
    "    return text\n",
    "\n",
    "# Функція лематизації\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "#до обох стовпців\n",
    "def process_text_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = clean_text(df[col])\n",
    "        df[col] = remove_stopwords(df[col])\n",
    "        delete = pd.Series(' '.join(df[col]).split()).value_counts()[-1000:]\n",
    "        df[col] = df[col].apply(lambda x: \" \".join(word for word in x.split() if word.lower() not in delete))\n",
    "        df[col] = df[col].apply(lemmatize_sentence)\n",
    "    return df\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df = process_text_columns(df, ['title', 'main_text'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:53:39.301684Z",
     "iopub.status.busy": "2024-12-22T17:53:39.301305Z",
     "iopub.status.idle": "2024-12-22T17:53:39.332857Z",
     "shell.execute_reply": "2024-12-22T17:53:39.332011Z",
     "shell.execute_reply.started": "2024-12-22T17:53:39.301656Z"
    },
    "id": "MeJKRkzHEazF",
    "outputId": "93935e7e-5357-4800-91b1-a70531dce804",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розподіл класів після балансування:\n",
      "label\n",
      "0    3257\n",
      "1    3257\n",
      "2    3257\n",
      "3    3257\n",
      "4    3257\n",
      "5    3257\n",
      "6    3257\n",
      "7    3257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Розподіл класів у тренувальному наборі:\n",
      "label\n",
      "5    2624\n",
      "3    2622\n",
      "7    2616\n",
      "6    2615\n",
      "0    2615\n",
      "4    2604\n",
      "1    2586\n",
      "2    2562\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Розподіл класів у тестовому наборі:\n",
      "label\n",
      "2    695\n",
      "1    671\n",
      "4    653\n",
      "0    642\n",
      "6    642\n",
      "7    641\n",
      "3    635\n",
      "5    633\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# основний текст\n",
    "# df['text'] = df['title'].str.strip(':') + ' ' + df['main_text']\n",
    "df['text'] = df['main_text']\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_res, y_res = undersampler.fit_resample(X.values.reshape(-1, 1), y)\n",
    "print(\"Розподіл класів після балансування:\")\n",
    "print(pd.Series(y_res).value_counts())\n",
    "balanced_df = pd.DataFrame({\n",
    "    'text': X_res.flatten(),\n",
    "    'label': y_res\n",
    "})\n",
    "\n",
    "# тренувальний та тестовий набори\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nРозподіл класів у тренувальному наборі:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\nРозподіл класів у тестовому наборі:\")\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "23088d4686a847d888be2d7da3950dd7",
      "c656045048814e748ec3e38d6d59d4a1",
      "cd1b1cfceaae423a9c9eeb0263ed29ee",
      "79720c2d9e144501a8840750aaad761a",
      "188ec33294214b169f1b96cb94a942b3"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-22T17:55:45.409322Z",
     "iopub.status.busy": "2024-12-22T17:55:45.408789Z",
     "iopub.status.idle": "2024-12-22T18:47:13.310833Z",
     "shell.execute_reply": "2024-12-22T18:47:13.309922Z",
     "shell.execute_reply.started": "2024-12-22T17:55:45.409278Z"
    },
    "id": "3LEsG0N9EazI",
    "outputId": "366f59e1-689a-4878-9c8b-67168431ea57",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23088d4686a847d888be2d7da3950dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c656045048814e748ec3e38d6d59d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1b1cfceaae423a9c9eeb0263ed29ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79720c2d9e144501a8840750aaad761a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188ec33294214b169f1b96cb94a942b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 1303/1303 [16:15<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.9528, Accuracy: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1303/1303 [16:30<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 0.6108, Accuracy: 0.8008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1303/1303 [16:29<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 0.4471, Accuracy: 0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 326/326 [01:23<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Transformer Model Evaluation:\n",
      "Accuracy: 0.782233307751343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       642\n",
      "           1       0.73      0.74      0.74       671\n",
      "           2       0.77      0.69      0.73       695\n",
      "           3       0.72      0.70      0.71       635\n",
      "           4       0.67      0.70      0.68       653\n",
      "           5       0.96      0.98      0.97       633\n",
      "           6       0.80      0.82      0.81       642\n",
      "           7       0.78      0.80      0.79       641\n",
      "\n",
      "    accuracy                           0.78      5212\n",
      "   macro avg       0.78      0.78      0.78      5212\n",
      "weighted avg       0.78      0.78      0.78      5212\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_data(texts, tokenizer, max_len):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "\n",
    "max_len = 256\n",
    "train_encodings = tokenize_data(train_df['text'].tolist(), tokenizer, max_len)\n",
    "test_encodings = tokenize_data(test_df['text'].tolist(), tokenizer, max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['label'])\n",
    "y_test = label_encoder.transform(test_df['label'])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train)\n",
    "test_dataset = TextDataset(test_encodings, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "def train_model(model, train_loader, optimizer, scheduler, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions += (predictions == batch['labels']).sum().item()\n",
    "            total_predictions += batch['labels'].size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, optimizer, scheduler, device, epochs)\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "y_test_pred, y_test_labels = evaluate_model(model, test_loader, device)\n",
    "print(\"BERT Transformer Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_labels, y_test_pred)}\")\n",
    "print(classification_report(y_test_labels, y_test_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
